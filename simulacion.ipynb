{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sembradora 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It demonstrates how to use the [agentpy](https://agentpy.readthedocs.io) package to create and visualize networks, use the interactive module, and perform different types of sensitivity analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model design\n",
    "import agentpy as ap\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from queue import PriorityQueue\n",
    "from itertools import count\n",
    "import math\n",
    "\n",
    "# Visualization\n",
    "import imageio\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Guardar\n",
    "import pickle\n",
    "\n",
    "# Sistema\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "1 is tractor\n",
    "2 is obstacle\n",
    "3 is target\n",
    "4 is seeds\n",
    "\"\"\"\n",
    "\n",
    "def is_connected(grid, free_positions):\n",
    "    \"\"\"Check if all free cells are connected using BFS.\"\"\"\n",
    "    n = grid.shape[0]\n",
    "    visited = set()\n",
    "    queue = deque([free_positions.pop()])\n",
    "    visited.add(queue[0])\n",
    "\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    \n",
    "    connected_count = 0\n",
    "    free_count = len(free_positions)\n",
    "\n",
    "    while queue:\n",
    "        x, y = queue.popleft()\n",
    "        for dx, dy in directions:\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            if (0 <= new_x < n and 0 <= new_y < n and \n",
    "                (new_x, new_y) in free_positions and (new_x, new_y) not in visited):\n",
    "                queue.append((new_x, new_y))\n",
    "                visited.add((new_x, new_y))\n",
    "                connected_count += 1\n",
    "                \n",
    "    return connected_count == free_count\n",
    "\n",
    "def is_adjacent(pos1, pos2):\n",
    "    return abs(pos1[0] - pos2[0]) <= 1 and abs(pos1[1] - pos2[1]) <= 1\n",
    "\n",
    "def generate_grid(model, n, obstacles_count, crops_count):\n",
    "    \"\"\"Generate a grid with obstacles, ensuring free cells are connected.\"\"\"\n",
    "    while True:\n",
    "        grid = ap.Grid(model, (n, n), track_empty=True)\n",
    "        grid.add_field(\"occupied\", 0)\n",
    "\n",
    "        obstacle_positions = set()\n",
    "        while len(obstacle_positions) < obstacles_count:\n",
    "            pos = (random.randint(0, n-1), random.randint(0, n-1))\n",
    "            if pos not in obstacle_positions:\n",
    "                # Check if pos is adjacent to any seed position\n",
    "                is_valid = True\n",
    "                for seed_pos in model.p.seedsPositions:\n",
    "                    if is_adjacent(pos, seed_pos):\n",
    "                        is_valid = False\n",
    "                        break\n",
    "                if is_valid:\n",
    "                    obstacle_positions.add(pos)\n",
    "\n",
    "        for pos in obstacle_positions:\n",
    "            grid[\"occupied\"][pos] = 1\n",
    "\n",
    "        free_positions = {(x, y) for x in range(n) for y in range(n) if grid[\"occupied\"][x, y] == 0}\n",
    "        \n",
    "        if is_connected(grid, free_positions):\n",
    "            break\n",
    "\n",
    "    for pos in obstacle_positions:\n",
    "        grid[\"occupied\"][pos] = 1\n",
    "        model.np_grid[pos] = 2\n",
    "        \n",
    "        crops_positions = set()\n",
    "        while len(crops_positions) < crops_count:\n",
    "            pos = (random.randint(0, n-1), random.randint(0, n-1))\n",
    "            if pos not in obstacle_positions and pos not in crops_positions:\n",
    "                crops_positions.add(pos)\n",
    "                model.np_grid[pos] = 3\n",
    "\n",
    "    agentlist = ap.AgentList(model, len(obstacle_positions), agent_type=2)\n",
    "    grid.add_agents(agentlist, obstacle_positions)\n",
    "    \n",
    "    model.grid = grid\n",
    "    \n",
    "    return obstacle_positions, crops_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectingTractor(ap.Agent):\n",
    "    def setup(self):\n",
    "        \"\"\"Initialize agent parameters.\"\"\"\n",
    "        self.initialized = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset agent attributes.\"\"\"\n",
    "        if not self.initialized:\n",
    "            self._initialize_agent()\n",
    "        else:\n",
    "            self._restore_agent()\n",
    "\n",
    "    def _initialize_agent(self):\n",
    "        \"\"\"Initialize the agent's initial state.\"\"\"\n",
    "        self.collected = 0\n",
    "        self.destroyed = False\n",
    "        self.condition = True\n",
    "        self.seeds = 0\n",
    "        self.type = 1\n",
    "        self.pos = (0, 0)\n",
    "        self.visited_positions = set()\n",
    "\n",
    "        # Initialize Q-learning parameters\n",
    "        self.q_table = np.zeros((self.p.grid_size, self.p.grid_size, 6))\n",
    "        self.learning_rate = self.p.learning_rate\n",
    "        self.discount_factor = self.p.discount_factor\n",
    "        self.epsilon = self.p.epsilon\n",
    "\n",
    "        self.initialized = True  # Mark as initialized\n",
    "\n",
    "    def _restore_agent(self):\n",
    "        \"\"\"Restore the agent's state to its initial conditions.\"\"\"\n",
    "        self.collected = 0\n",
    "        self.targetIndex = 1\n",
    "        self.destroyed = False\n",
    "        self.condition = True\n",
    "        self.seeds = 0\n",
    "        self.type = 1\n",
    "        self.pos = (0, 0)\n",
    "        self.visited_positions = set()\n",
    "\n",
    "        # Initialize Q-learning parameters\n",
    "        self.q_table = np.zeros((self.p.grid_size, self.p.grid_size, 6))\n",
    "        self.learning_rate = self.p.learning_rate\n",
    "        self.discount_factor = self.p.discount_factor\n",
    "        self.epsilon = self.p.epsilon\n",
    "\n",
    "    def move(self, dx=0, dy=0):\n",
    "        \"\"\"Move the tractor by (dx, dy).\"\"\"\n",
    "        x, y = self.pos\n",
    "        new_pos = (x + dx, y + dy)\n",
    "        \n",
    "        if 0 <= new_pos[0] < self.p.grid_size and 0 <= new_pos[1] < self.p.grid_size:\n",
    "            if self.model.np_grid[new_pos] != 2:\n",
    "                self.pos = new_pos\n",
    "                return self.pos, True\n",
    "            else:\n",
    "                return self.pos, False\n",
    "        return self.pos, False\n",
    "\n",
    "    def move_up(self):\n",
    "        return self.move(dy=1)\n",
    "\n",
    "    def move_down(self):\n",
    "        return self.move(dy=-1)\n",
    "\n",
    "    def move_left(self):\n",
    "        return self.move(dx=-1)\n",
    "\n",
    "    def move_right(self):\n",
    "        return self.move(dx=1)\n",
    "\n",
    "    def drop(self):\n",
    "        \"\"\"Drop seeds at the current target position.\"\"\"\n",
    "        if self.seeds > 0 and self.pos and self.model.np_grid[self.pos] == 3:\n",
    "            self.seeds -= 1\n",
    "            # Remove target from grid\n",
    "            self.model.np_grid[self.pos] = 0\n",
    "            return self.pos, True\n",
    "        else:\n",
    "            return self.pos, False\n",
    "\n",
    "    def collect(self):\n",
    "        \"\"\"Collect seeds if available at the current position.\"\"\"\n",
    "        if self.model.np_grid[self.pos] == 4 and self.seeds == 0:\n",
    "            self.seeds = self.capacity\n",
    "            return self.pos, 1\n",
    "        elif self.model.np_grid[self.pos] == 4 and self.seeds >= 0 and self.seeds < self.capacity:\n",
    "            self.seeds = self.capacity\n",
    "            return self.pos, 2\n",
    "        else:\n",
    "            return self.pos, 3\n",
    "\n",
    "    def get_reward(self, action, agents, accionado):\n",
    "        \"\"\"Calculate the reward based on the action and outcome.\"\"\"\n",
    "        reward = 0  # No base penalty for movement\n",
    "\n",
    "        if action in [0, 1, 2, 3]:  # Movement actions\n",
    "            if accionado:\n",
    "                reward += 1  # Small reward for valid movement\n",
    "            else:\n",
    "                reward -= 10  # Strong penalty for hitting an obstacle or invalid move\n",
    "        elif action == 4:  # Drop seeds\n",
    "            if accionado:\n",
    "                reward += 25 \n",
    "            else:\n",
    "                reward -= 1\n",
    "        elif action == 5:  # Collect seeds\n",
    "            if accionado == 1:\n",
    "                reward += 20\n",
    "            elif accionado == 2:\n",
    "                reward += 10\n",
    "            else:\n",
    "                reward -= 1\n",
    "    \n",
    "        # Penalty for collision with other agents, not self\n",
    "        for agent in agents:\n",
    "            if agent != self and agent.pos == self.pos:\n",
    "                reward -= 10\n",
    "\n",
    "        # Penalty for collision with obstacles\n",
    "        if self.model.np_grid[self.pos] == 2:\n",
    "            reward -= 10\n",
    "\n",
    "        # Reward for exploring new positions\n",
    "        if self.pos not in self.visited_positions:\n",
    "            reward += 1\n",
    "            self.visited_positions.add(self.pos)\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def q_learning_update(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-table using the Q-learning algorithm.\"\"\"\n",
    "        x, y = state\n",
    "        next_x, next_y = next_state\n",
    "        \n",
    "        # Ensure the state and next_state are within the grid bounds\n",
    "        if 0 <= x < self.p.grid_size and 0 <= y < self.p.grid_size and 0 <= next_x < self.p.grid_size and 0 <= next_y < self.p.grid_size:\n",
    "            current_q = self.q_table[x, y, action]\n",
    "            max_next_q = np.max(self.q_table[next_x, next_y])\n",
    "            new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
    "            self.q_table[x, y, action] = new_q\n",
    "\n",
    "    def acciones(self, action):\n",
    "        \"\"\"Map action to the corresponding method.\"\"\"\n",
    "        action_map = {\n",
    "            0: self.move_up,\n",
    "            1: self.move_down,\n",
    "            2: self.move_left,\n",
    "            3: self.move_right,\n",
    "            4: self.drop,\n",
    "            5: self.collect\n",
    "        }\n",
    "        return action_map[action]()\n",
    "\n",
    "    def step(self, agents):\n",
    "        \"\"\"Execute a step in the agent's behavior.\"\"\"\n",
    "        if self.destroyed:\n",
    "            return\n",
    "\n",
    "        state = self.pos\n",
    "        x, y = state\n",
    "        \n",
    "        # Action selection using epsilon-greedy\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice([0, 1, 2, 3, 4, 5])\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[x, y])\n",
    "        \n",
    "        # Execute the action and update Q-table\n",
    "        next_state, accionado = self.acciones(action)\n",
    "        \n",
    "        # Ensure next_state is within bounds\n",
    "        if 0 <= next_state[0] < self.p.grid_size and 0 <= next_state[1] < self.p.grid_size:\n",
    "            reward = self.get_reward(action, agents, accionado)\n",
    "            self.q_learning_update(state, action, reward, next_state)\n",
    "        else:\n",
    "            # Optional: Handle invalid state transitions, if necessary\n",
    "            pass\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TractorModel(ap.Model):\n",
    "    def setup(self):\n",
    "        \"\"\"Initialize the environment and generate coordinates.\"\"\"\n",
    "        self.initialized = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset or initialize the model state.\"\"\"\n",
    "        if not self.initialized:\n",
    "            self._initialize_environment()\n",
    "        else:\n",
    "            self._restore_environment()\n",
    "\n",
    "        self.steps = 0  # Reset the step counter\n",
    "\n",
    "    def _initialize_environment(self):\n",
    "        \"\"\"Initialize the environment, agents, and their targets.\"\"\"\n",
    "        self.Collected = 0\n",
    "        self.steps = 0\n",
    "        self.max_steps = self.p.max_steps\n",
    "        self.np_grid = np.zeros((self.p.grid_size, self.p.grid_size))\n",
    "\n",
    "        # Generate and store obstacle positions\n",
    "        obastacle_pos, crop_pos = generate_grid(self, self.p.grid_size, self.p.obstacles_count, self.p.crops_count)\n",
    "        self.obstacle_positions = np.copy(self.grid[\"occupied\"])\n",
    "        self.crops_pos = crop_pos   \n",
    "        \n",
    "        # Create agents and assign initial properties\n",
    "        self.agents = ap.AgentList(self, self.p.number_of_tractors, CollectingTractor)\n",
    "        self._initialize_agents(obastacle_pos, crop_pos)\n",
    "\n",
    "        self.grid.add_agents(self.agents, [tractor.pos for tractor in self.agents])\n",
    "        \n",
    "        for seed in self.p.seedsPositions:\n",
    "            self.np_grid[seed] = 4\n",
    "            \n",
    "        self.initialized = True  # Mark as initialized\n",
    "        \n",
    "    def _restore_environment(self):\n",
    "        \"\"\"Restore the environment to its initial state.\"\"\"\n",
    "        self.Collected = 0\n",
    "        self.steps = 0\n",
    "        self.max_steps = self.p.max_steps\n",
    "        self.np_grid = np.zeros((self.p.grid_size, self.p.grid_size))\n",
    "        \n",
    "        # Restore obstacle positions\n",
    "        for x in range(self.p.grid_size):\n",
    "            for y in range(self.p.grid_size):\n",
    "                if self.obstacle_positions[x, y] == 1:\n",
    "                    self.np_grid[x, y] = 2 \n",
    "        \n",
    "        # Restore crop positions\n",
    "        for pos in self.crops_pos:\n",
    "            self.np_grid[pos] = 3\n",
    "            \n",
    "        self.agents.capacity = self.p.capacity\n",
    "        self.agents.seeds = self.p.starting_seeds\n",
    "        self.agents.destroyed = False\n",
    "        self.grid.add_agents(self.agents, [tractor.pos for tractor in self.agents])\n",
    "        \n",
    "        for seed in self.p.seedsPositions:\n",
    "            self.np_grid[seed] = 4\n",
    "\n",
    "    def _initialize_agents(self, obstacle_pos, crop_pos):\n",
    "        \"\"\"Initialize agents with positions and targets.\"\"\"\n",
    "        self.agents.capacity = self.p.capacity\n",
    "        self.agents.seeds = self.p.starting_seeds\n",
    "        self.agents.destroyed = False\n",
    "\n",
    "        self.coordsUsed = set()\n",
    "        self.coordsUsed.update(obstacle_pos)\n",
    "        self.coordsUsed.update(crop_pos)\n",
    "        self.targets_by_tractor = []\n",
    "        \n",
    "    def _get_free_position(self):\n",
    "        \"\"\"Get a free position on the grid.\"\"\"\n",
    "        while True:\n",
    "            x, y = random.randint(0, self.p.grid_size - 1), random.randint(0, self.p.grid_size - 1)\n",
    "            if self.grid[\"occupied\"][(x, y)] == 0 and (x, y) not in self.coordsUsed and (x, y) != (0, 0):\n",
    "                return (x, y)\n",
    "\n",
    "    def step(self):\n",
    "        total_reward = 0\n",
    "        for tractor in self.agents:\n",
    "            reward = tractor.step(self.agents)\n",
    "            total_reward += reward\n",
    "        if self.steps >= self.max_steps:\n",
    "            for tractor in self.agents:\n",
    "                tractor.destroyed = True\n",
    "            self.end()\n",
    "        self.steps += 1\n",
    "        return total_reward\n",
    "\n",
    "    def end(self):\n",
    "        self.report('Total targets', self.agents.collected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "0 is empty\n",
    "1 is tractor\n",
    "2 is obstacle\n",
    "3 is target\n",
    "4 is seeds\n",
    "\"\"\"\n",
    "\n",
    "# Load images\n",
    "tractor_img = mpimg.imread('tractor.png')\n",
    "obstacle_img = mpimg.imread('obstacle.png')\n",
    "target_img = mpimg.imread('target.png')\n",
    "seeds_img = mpimg.imread('seeds.png')\n",
    "empty_img = mpimg.imread('empty.png')\n",
    "\n",
    "def save_frame(model, filename, total_reward):\n",
    "    fig, ax = plt.subplots()\n",
    "    # Clear the axis to avoid over-plotting\n",
    "    ax.clear()\n",
    "\n",
    "    # Plot the grid using images\n",
    "    for (x, y), value in np.ndenumerate(model.np_grid):\n",
    "        if value == 2:  # Obstacle\n",
    "            ax.imshow(obstacle_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "        elif value == 3:  # Target\n",
    "            ax.imshow(target_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "        elif value == 4:  # Seeds\n",
    "            ax.imshow(seeds_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "        else:\n",
    "            ax.imshow(empty_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "            \n",
    "    # Add text for each tractor displaying the number of seeds it has\n",
    "    for agent in model.agents:\n",
    "        if agent.destroyed:\n",
    "            continue\n",
    "        ax.imshow(tractor_img, extent=[agent.pos[1], agent.pos[1]+1, agent.pos[0], agent.pos[0]+1], aspect='auto')\n",
    "        ax.text(agent.pos[1] + 0.5, agent.pos[0] + 0.5, str(agent.seeds),\n",
    "                color='black', fontsize=12, ha='center', va='center', weight='bold')\n",
    "\n",
    "    # Fix axis limits based on the grid dimensions\n",
    "    ax.set_xlim([0, model.np_grid.shape[1]])\n",
    "    ax.set_ylim([0, model.np_grid.shape[0]])\n",
    "\n",
    "    # Set aspect ratio to 'equal' to prevent image stretching\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Set the title for the plot\n",
    "    ax.set_title(f\"Tractor model \\n Time-step: {model.steps} - Total reward: {total_reward}\")\n",
    "\n",
    "    # Save the frame\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def create_gif(filenames, gif_filename):\n",
    "    with imageio.get_writer(gif_filename, mode='I', duration=0.1) as writer:\n",
    "        for filename in filenames:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "    \n",
    "def delete_temp_frames(prefix):\n",
    "    for filename in os.listdir():\n",
    "        if prefix in filename:\n",
    "            os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_crops_count(grid_size, obstacles_count):\n",
    "    return grid_size**2 - obstacles_count\n",
    "\n",
    "tractorParameters = {\n",
    "    'grid_size': 10,\n",
    "    'obstacles_count': 10,\n",
    "    'number_of_tractors': 2,\n",
    "    'max_steps': 100,\n",
    "    'seedsPositions': [(5, 5)],\n",
    "    'capacity': 2,\n",
    "    'starting_seeds': 2,\n",
    "    'learning_rate': 0.9,\n",
    "    'discount_factor': 0.99,\n",
    "    'epsilon': 1.0,\n",
    "    'crops_count': calculate_crops_count(10, 10)  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "rewards_per_episode = []\n",
    "epsilon_values = []\n",
    "\n",
    "# Inicializar el modelo\n",
    "model = TractorModel(tractorParameters)\n",
    "model.setup()\n",
    "\n",
    "num_episodes = 1000\n",
    "\n",
    "def run_episode(episode, save_gif=False):\n",
    "    model.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    # Configura epsilon basado en el progreso de los episodios\n",
    "    epsilon_start = 0.9  # Valor inicial de epsilon\n",
    "    epsilon_end = 0.01    # Valor final de epsilon\n",
    "    epsilon_decay_rate = epsilon_start - epsilon_end\n",
    "    epsilon = max(epsilon_end, epsilon_start - (epsilon_decay_rate * episode / (num_episodes / 2)))\n",
    "    \n",
    "    for tractor in model.agents:\n",
    "        tractor.epsilon = epsilon\n",
    "\n",
    "    filenames = []\n",
    "    while model.steps < model.max_steps:\n",
    "        total_reward = model.step()  # Ejecutar un paso en el modelo\n",
    "        \n",
    "        if all([tractor.destroyed for tractor in model.agents]):\n",
    "            break\n",
    "\n",
    "        if save_gif:\n",
    "            filename = f'frame_{model.steps}.png'\n",
    "            save_frame(model, filename, total_reward)\n",
    "            filenames.append(filename)\n",
    "    \n",
    "    if save_gif:\n",
    "        gif_filename = '1rstSimulacionTractores.gif' if episode == 0 else 'lastSimulacionTractores.gif'\n",
    "        create_gif(filenames, gif_filename)\n",
    "        delete_temp_frames('frame')\n",
    "\n",
    "    return total_reward, epsilon\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    save_gif = (episode == 0 or episode == num_episodes - 1)\n",
    "    total_reward, epsilon = run_episode(episode, save_gif)\n",
    "    \n",
    "    rewards_per_episode.append(total_reward)\n",
    "    epsilon_values.append(epsilon)\n",
    "\n",
    "    # Monitorear el progreso cada 10 episodios\n",
    "    if episode % 10 == 0 or episode == num_episodes - 1:\n",
    "        print(f'Episodio {episode + 1}/{num_episodes}, Recompensa total: {total_reward:.2f}, Epsilon: {epsilon:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar la Q-table al final del entrenamiento\n",
    "q_table = model.agents[0].q_table  # Asumiendo que todos los agentes comparten la misma Q-table\n",
    "with open('q_table.pkl', 'wb') as f:\n",
    "    pickle.dump(q_table, f)\n",
    "\n",
    "# Graficar el rendimiento\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_per_episode, label='Total Reward per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Reward per Episode')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Asignar la Q-table cargada a los agentes\n",
    "model = TractorModel(tractorParameters)\n",
    "model.setup()\n",
    "\n",
    "for agent in model.agents:\n",
    "    agent.q_table = q_table  # Asignar la Q-table cargada a cada agente\n",
    "\n",
    "# Ahora puedes continuar entrenando o simular el comportamiento\n",
    "model.reset()  # Resetear el ambiente si es necesario\n",
    "model.step()  # Ejecutar pasos del modelo utilizando la Q-table cargada\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

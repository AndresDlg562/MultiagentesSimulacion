{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sembradora 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents an agent-based model that simulates the propagation of a disease through a network.\n",
    "It demonstrates how to use the [agentpy](https://agentpy.readthedocs.io) package to create and visualize networks, use the interactive module, and perform different types of sensitivity analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model design\n",
    "import agentpy as ap\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from queue import PriorityQueue\n",
    "from itertools import count\n",
    "import math\n",
    "\n",
    "# Visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Guardar\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the model\n",
    "\n",
    "The agents of this model are people, which can be in one of the following three conditions: susceptible to the disease (S), infected (I), or recovered (R). The agents are connected to each other through a small-world network of peers. At every time-step, infected agents can infect their peers or recover from the disease based on random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1 is tractor\n",
    "2 is obstacle\n",
    "3 is target\n",
    "4 is seeds\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def is_connected(grid, free_positions):\n",
    "    \"\"\" Check if all free cells are connected using BFS \"\"\"\n",
    "    n = grid.shape[0]\n",
    "    visited = set()\n",
    "    queue = deque([free_positions.pop()])\n",
    "    visited.add(queue[0])\n",
    "\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    \n",
    "    connected_count = 1\n",
    "    free_count = len(free_positions)\n",
    "\n",
    "    while queue:\n",
    "        x, y = queue.popleft()\n",
    "        for dx, dy in directions:\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            if 0 <= new_x < n and 0 <= new_y < n and (new_x, new_y) in free_positions and (new_x, new_y) not in visited:\n",
    "                queue.append((new_x, new_y))\n",
    "                visited.add((new_x, new_y))\n",
    "                connected_count += 1\n",
    "        \n",
    "                \n",
    "    return connected_count > free_count\n",
    "\n",
    "def generate_grid(model, n, obstacles_count):\n",
    "    grid = ap.Grid(model, (n, n), track_empty=True)  # Create an agentpy Grid object\n",
    "    grid.add_field(\"occupied\", 0)  # Add a field to store obstacle information\n",
    "\n",
    "    # Generate obstacle positions\n",
    "    obstacle_positions = set()\n",
    "    while len(obstacle_positions) < obstacles_count:\n",
    "        x, y = random.randint(0, n-1), random.randint(0, n-1)\n",
    "        if (x, y) not in obstacle_positions and (x,y) not in model.p.seedsPositions:\n",
    "            obstacle_positions.add((x, y))\n",
    "\n",
    "    # Mark grid cells as obstacles\n",
    "    for pos in obstacle_positions:\n",
    "        grid[\"occupied\"][pos] = 1\n",
    "\n",
    "    # Identify free positions\n",
    "    free_positions = set()\n",
    "    for pos in grid.all:\n",
    "        if grid[\"occupied\"][pos] != 1:\n",
    "            free_positions.add(pos)\n",
    "\n",
    "    # Check if the free cells are connected\n",
    "    final_obstacles = set()\n",
    "    while not is_connected(grid, free_positions):\n",
    "        final_obstacles.clear()\n",
    "        grid = ap.Grid(model, (n, n), track_empty=True)\n",
    "        grid.add_field(\"occupied\", 0)\n",
    "        obstacle_positions = set()\n",
    "        while len(obstacle_positions) < obstacles_count:\n",
    "            x, y = random.randint(0, n-1), random.randint(0, n-1)\n",
    "            if (x, y) not in obstacle_positions and (x,y) not in model.p.seedsPositions:\n",
    "                obstacle_positions.add((x, y))\n",
    "        \n",
    "        free_positions = set()\n",
    "        for pos in grid.all:\n",
    "            if grid[\"occupied\"][pos] == 0:\n",
    "                free_positions.add(pos)\n",
    "        final_obstacles = obstacle_positions\n",
    "\n",
    "    for pos in obstacle_positions:\n",
    "        grid[\"occupied\"][pos] = 1\n",
    "        model.np_grid[pos] = 3\n",
    "    #add an agent to each obstacle position with type 2\n",
    "    #Make an agentlist ap.agentlist\n",
    "    #Add the agent to the grid\n",
    "    #Add the agent to the agentlist\n",
    "    agentlist = ap.AgentList(model, len(obstacle_positions))\n",
    "    agentlist.type = 2\n",
    "        \n",
    "    grid.add_agents(agentlist, obstacle_positions)\n",
    "    \n",
    "\n",
    "\n",
    "    model.grid = grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectingTractor(ap.Agent):\n",
    "    def setup(self, type=1, pos=(0, 0)):\n",
    "        self.collected = 0\n",
    "        self.targetIndex = 1\n",
    "        self.path = []\n",
    "        self.destroyed = False\n",
    "        self.condition = True\n",
    "        self.seeds = 0\n",
    "        self.type = type\n",
    "        \n",
    "        # Machine Learning\n",
    "        self.start = pos\n",
    "        self.q_table = np.zeros((self.p.grid_size, self.p.grid_size, 6))  # 6 acciones posibles\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.1  \n",
    "        self.pos = pos  # Posición inicial\n",
    "        \n",
    "    # Función para moverse arrriba\n",
    "    def move_up(self):\n",
    "        x, y = self.pos\n",
    "        if y < self.p.grid_size - 1:\n",
    "            self.model.grid.move_to(self, (x, y + 1))\n",
    "            self.pos = (x, y + 1)\n",
    "            accionado = True\n",
    "            return self.pos, accionado\n",
    "        else:\n",
    "            accionado = False\n",
    "            return self.pos, accionado\n",
    "        \n",
    "    # Función para moverse abajo\n",
    "    def move_down(self):\n",
    "        x, y = self.pos\n",
    "        if y > 0:\n",
    "            self.model.grid.move_to(self, (x, y - 1))\n",
    "            self.pos = (x, y - 1)\n",
    "            accionado = True\n",
    "            return self.pos, accionado\n",
    "        else:\n",
    "            accionado = False\n",
    "            return self.pos, accionado\n",
    "        \n",
    "    # Función para moverse a la izquierda\n",
    "    def move_left(self):\n",
    "        x, y = self.pos\n",
    "        if x > 0:\n",
    "            self.model.grid.move_to(self, (x - 1, y))\n",
    "            self.pos = (x - 1, y)\n",
    "            accionado = True\n",
    "            return self.pos, accionado \n",
    "        else:\n",
    "            accionado = False\n",
    "            return self.pos, accionado\n",
    "        \n",
    "    # Función para moverse a la derecha\n",
    "    def move_right(self):\n",
    "        x, y = self.pos\n",
    "        if x < self.p.grid_size - 1:\n",
    "            self.model.grid.move_to(self, (x + 1, y))\n",
    "            self.pos = (x + 1, y)\n",
    "            accionado = True\n",
    "            return self.pos, accionado\n",
    "        else:\n",
    "            accionado = False\n",
    "            return self.pos, accionado\n",
    "    \n",
    "    # función para dejar una semilla\n",
    "    def drop(self):\n",
    "        x, y = self.pos\n",
    "        if self.seeds > 0 and self.pos == self.targets[self.targetIndex]:\n",
    "            self.targetIndex += 1\n",
    "            self.seeds -= 1\n",
    "            if (self.targetIndex >= len(self.targets)):\n",
    "                self.destroyed = True\n",
    "            accionado = True\n",
    "            return self.pos, accionado\n",
    "        else:\n",
    "            accionado = False\n",
    "            return self.pos, accionado\n",
    "            \n",
    "    # función para recoger una semillas\n",
    "    def collect(self):\n",
    "        x, y = self.pos\n",
    "        if self.model.np_grid[self.pos] == 4 and self.seeds == 0: # If semillas == 0, agarra semilas\n",
    "            self.seeds = self.capacity\n",
    "            accionado = 1\n",
    "            return self.pos, accionado\n",
    "        elif self.model.np_grid[self.pos] == 4 and self.seeds < self.capacity: # If semillas < capacidad maxima, agarra semillas\n",
    "            self.seeds = self.capacity\n",
    "            accionado = 2\n",
    "            return self.pos, accionado\n",
    "        else: # Si semillas == capacidad maxima, no agarra semillas\n",
    "            accionado = 3\n",
    "            return self.pos, accionado\n",
    "    \n",
    "    \n",
    "    def get_reward(self, action, agents, accionado):\n",
    "        if action == 0:  # Arriba\n",
    "            if accionado == True:\n",
    "                reward = -5  # Penalización por step\n",
    "            else:\n",
    "                reward = -15  # Penalización por intentar salir del límite\n",
    "        elif action == 1:  # Abajo\n",
    "            if accionado == True:\n",
    "                reward = -5\n",
    "            else:\n",
    "                reward = -15\n",
    "        elif action == 2:  # Izquierda\n",
    "            if accionado == True:\n",
    "                reward = -5\n",
    "            else:\n",
    "                reward = -15\n",
    "        elif action == 3:  # Derecha\n",
    "            if accionado == True:\n",
    "                reward = -5\n",
    "            else:\n",
    "                reward = -15\n",
    "        elif action == 4:  # Dejar\n",
    "            if accionado == True:\n",
    "                reward = 10  # Recompensa positiva por dejar semillas\n",
    "            else:\n",
    "                reward = -15  # Recompensa negativa por intentar dejar sin semillas\n",
    "        elif action == 5:  # Recoger\n",
    "            if accionado == 1:\n",
    "                reward = -5\n",
    "            elif accionado == 2:\n",
    "                reward = -10\n",
    "            else:\n",
    "                reward = -15\n",
    "        else:\n",
    "            # Penalización pequeña por cada movimiento para incentivar la eficiencia\n",
    "            reward = -10\n",
    "\n",
    "        # Verificación de colisiones con otros agentes\n",
    "        for agent in agents:\n",
    "            if agent != self and agent.pos == self.pos:\n",
    "                reward -= 50  # Penalización por colisión con otro agente\n",
    "                break\n",
    "        \n",
    "        # Verificación de colisiones con obstáculos\n",
    "        if self.model.np_grid[self.pos] == 2:\n",
    "            reward -= 100\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def q_learning_update(self, state, action, reward, next_state):\n",
    "        # Desempaqueta las coordenadas de estado y next_state\n",
    "        x, y = state\n",
    "        next_x, next_y = next_state\n",
    "\n",
    "        # Accede al valor Q actual\n",
    "        current_q = self.q_table[x, y, action]\n",
    "\n",
    "        # Encuentra el valor máximo de Q para el siguiente estado\n",
    "        max_next_q = np.max(self.q_table[next_x, next_y])\n",
    "\n",
    "        # Calcula el nuevo valor Q\n",
    "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
    "\n",
    "        # Actualiza la Q-table\n",
    "        self.q_table[x, y, action] = new_q\n",
    "\n",
    "\n",
    "    def acciones(self, action):\n",
    "        if action == 0:  # Arriba\n",
    "            return self.move_up()\n",
    "        elif action == 1:  # Abajo\n",
    "            return self.move_down()\n",
    "        elif action == 2:  # Izquierda\n",
    "            return self.move_left()\n",
    "        elif action == 3:  # Derecha\n",
    "            return self.move_right()\n",
    "        elif action == 4:  # Dejar\n",
    "            return self.drop()\n",
    "        elif action == 5:  # Recoger\n",
    "            return self.collect()\n",
    "\n",
    "\n",
    "    def step(self, agents):\n",
    "        state = self.pos  # Estado actual (x, y)\n",
    "        x, y = state\n",
    "        \n",
    "        # Selección de acción usando epsilon-greedy\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice([0, 1, 2, 3, 4, 5])  # Explorar\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[x, y])  # Explotar\n",
    "            \n",
    "        # Reduce epsilon (Exploración) con el tiempo\n",
    "        self.epsilon = max(0.1, self.epsilon * 0.995)  # Se asegura que epsilon no baje de 0.1\n",
    "\n",
    "        # Ejecuta la acción y obtiene el nuevo estado y el resultado\n",
    "        next_state, accionado = self.acciones(action)\n",
    "        \n",
    "        # Calcula la recompensa basada en la acción y el estado\n",
    "        reward = self.get_reward(action, agents, accionado)\n",
    "\n",
    "        # Actualiza la tabla Q\n",
    "        self.q_learning_update(state, action, reward, next_state)\n",
    "\n",
    "        # Actualiza la posición del agente\n",
    "        self.pos = next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TractorModel(ap.Model):\n",
    "    def setup(self):\n",
    "        # Inicializa el ambiente y guarda las coordenadas generadas\n",
    "        self.initialized = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Solo genera nuevas coordenadas la primera vez\n",
    "        if not self.initialized:\n",
    "            self.Collected = 0\n",
    "            self.np_grid = np.zeros((self.p.grid_size, self.p.grid_size))\n",
    "            \n",
    "            # Generar y guardar las posiciones de los obstáculos\n",
    "            generate_grid(self, self.p.grid_size, self.p.obstacles_count)\n",
    "            self.obstacle_positions = np.copy(self.grid[\"occupied\"])\n",
    "\n",
    "            # Re-crea la lista de agentes y sus propiedades\n",
    "            self.agents = ap.AgentList(self, self.p.number_of_tractors, CollectingTractor)\n",
    "            self.agents.capacity = self.p.capacity\n",
    "            self.agents.seeds = self.p.starting_seeds\n",
    "\n",
    "            # Asigna posiciones y objetivos a los tractores\n",
    "            self.coordsUsed = set()\n",
    "            self.targets_by_tractor = []\n",
    "            for tractor in self.agents:\n",
    "                targets = []\n",
    "                for i in range(self.p.number_of_targets + 1):\n",
    "                    x, y = random.randint(0, self.p.grid_size-1), random.randint(0, self.p.grid_size-1)\n",
    "                    while self.grid[\"occupied\"][(x, y)] == 1 or (x, y) in self.coordsUsed:\n",
    "                        x, y = random.randint(0, self.p.grid_size-1), random.randint(0, self.p.grid_size-1)\n",
    "                    targets.append((x, y))\n",
    "                    self.coordsUsed.add((x, y))\n",
    "                self.targets_by_tractor.append(targets)\n",
    "                tractor.targets = targets\n",
    "                tractor.pos = targets[0]\n",
    "                self.np_grid[tractor.pos] = 1\n",
    "\n",
    "            self.grid.add_agents(self.agents, [tractor.pos for tractor in self.agents])\n",
    "            self.initialized = True  # Marcar como inicializado\n",
    "        else:\n",
    "            # Restaurar las posiciones de obstáculos y objetivos previas\n",
    "            self.grid[\"occupied\"] = np.copy(self.obstacle_positions)\n",
    "            self.np_grid = np.zeros((self.p.grid_size, self.p.grid_size))\n",
    "            for tractor, targets in zip(self.agents, self.targets_by_tractor):\n",
    "                tractor.targets = targets\n",
    "                tractor.pos = targets[0]\n",
    "                self.np_grid[tractor.pos] = 1\n",
    "\n",
    "            self.grid.add_agents(self.agents, [tractor.pos for tractor in self.agents])\n",
    "\n",
    "    def update(self):\n",
    "        # Actualiza la cantidad de objetivos recolectados\n",
    "        self.record('Collected', sum([tractor.collected for tractor in self.agents]))\n",
    "\n",
    "    def step(self):\n",
    "        # Ejecuta un paso para todos los agentes\n",
    "        self.agents.step(self.agents)\n",
    "        # Marca las posiciones de las semillas en la grilla\n",
    "        for seed in self.p.seedsPositions:\n",
    "            self.np_grid[seed] = 4\n",
    "\n",
    "    def end(self):\n",
    "        # Reporta el total de objetivos al final de la simulación\n",
    "        self.report('Total targets', self.p.number_of_targets * self.p.number_of_tractors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tractorParameters = {\n",
    "    'grid_size': 10,\n",
    "    'obstacles_count': 10,\n",
    "    'number_of_tractors': 2,\n",
    "    'number_of_targets': 20,\n",
    "    'steps': 100,\n",
    "    'seedsPositions': [(0, 0)],\n",
    "    'capacity': 2,\n",
    "    'starting_seeds': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "0 is empty\n",
    "1 is tractor\n",
    "2 is obstacle\n",
    "3 is target\n",
    "4 is seeds\n",
    "\"\"\"\n",
    "\n",
    "# Load images\n",
    "tractor_img = mpimg.imread('tractor.png')\n",
    "obstacle_img = mpimg.imread('obstacle.png')\n",
    "target_img = mpimg.imread('target.png')\n",
    "seeds_img = mpimg.imread('seeds.png')\n",
    "\n",
    "def animation_plot(model, ax):\n",
    "    # Clear the axis to avoid over-plotting\n",
    "    ax.clear()\n",
    "\n",
    "    # Plot the grid using images\n",
    "    for (x, y), value in np.ndenumerate(model.np_grid):\n",
    "        if value == 1:  # Tractor\n",
    "            ax.imshow(tractor_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "        elif value == 3:  # Obstacle\n",
    "            ax.imshow(obstacle_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "        elif value == 2:  # Target\n",
    "            ax.imshow(target_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "        elif value == 4:  # Seeds\n",
    "            ax.imshow(seeds_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "\n",
    "    # Add text for each tractor displaying the number of seeds it has\n",
    "    for agent in model.agents:\n",
    "        if agent.destroyed:\n",
    "            continue\n",
    "        ax.text(agent.pos[1] + 0.5, agent.pos[0] + 0.5, str(agent.seeds),\n",
    "                color='black', fontsize=12, ha='center', va='center', weight='bold')\n",
    "\n",
    "    # Fix axis limits based on the grid dimensions\n",
    "    ax.set_xlim([0, model.np_grid.shape[1]])\n",
    "    ax.set_ylim([0, model.np_grid.shape[0]])\n",
    "\n",
    "    # Set aspect ratio to 'equal' to prevent image stretching\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Set the title for the plot\n",
    "    ax.set_title(f\"Tractor model \\n Time-step: {model.t}, Collected: {model.Collected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de parámetros\n",
    "num_episodes = 3  # Número de episodios para entrenar y evaluar\n",
    "rewards_per_episode = []\n",
    "epsilon_values = []\n",
    "\n",
    "# Crear una única instancia del modelo\n",
    "model = TractorModel(tractorParameters)\n",
    "model.setup()  # Configurar el modelo\n",
    "\n",
    "# Entrenamiento del agente\n",
    "for episode in range(num_episodes):\n",
    "    model.reset()  # Resetear el ambiente al inicio de cada episodio\n",
    "    \n",
    "    total_reward = 0\n",
    "\n",
    "    # Para la animación del último episodio\n",
    "    if episode == num_episodes - 1:\n",
    "        fig, ax = plt.subplots()\n",
    "        animation = ap.animate(model, fig, ax, animation_plot)\n",
    "\n",
    "    while True:\n",
    "        model.step()  # Ejecutar un paso del modelo\n",
    "        \n",
    "        # Acumular las recompensas obtenidas por todos los agentes\n",
    "        for agent in model.agents:\n",
    "            x, y = agent.pos\n",
    "            action = np.argmax(agent.q_table[x, y])  # Obtener la acción elegida\n",
    "            accionado = agent.acciones(action)[1]\n",
    "            reward = agent.get_reward(action, model.agents, accionado)  # Obtener recompensa\n",
    "            total_reward += reward  # Acumular la recompensa obtenida\n",
    "        \n",
    "        if model.end():  # Verificar si se cumple la condición de finalización del episodio\n",
    "            break\n",
    "\n",
    "    # Guardar la recompensa total del episodio y el valor de epsilon\n",
    "    rewards_per_episode.append(total_reward)\n",
    "    epsilon_values.append(model.agents[0].epsilon)  # Se asume que todos los agentes tienen el mismo epsilon\n",
    "    \n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}, Epsilon = {model.agents[0].epsilon}\")\n",
    "\n",
    "# Guardar la animación del último episodio como un GIF\n",
    "animation.save('simulacionTractores.gif')\n",
    "\n",
    "# Guardar la Q-table al final del entrenamiento\n",
    "q_table = model.agents[0].q_table  # Asumiendo que todos los agentes comparten la misma Q-table\n",
    "with open('q_table.pkl', 'wb') as f:\n",
    "    pickle.dump(q_table, f)\n",
    "\n",
    "# Graficar el rendimiento\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_per_episode, label='Total Reward per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Reward per Episode')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epsilon_values, label='Epsilon Value')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.title('Epsilon Decay over Episodes')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Asignar la Q-table cargada a los agentes\n",
    "model = TractorModel(tractorParameters)\n",
    "model.setup()\n",
    "\n",
    "for agent in model.agents:\n",
    "    agent.q_table = q_table  # Asignar la Q-table cargada a cada agente\n",
    "\n",
    "# Ahora puedes continuar entrenando o simular el comportamiento\n",
    "model.reset()  # Resetear el ambiente si es necesario\n",
    "model.step()  # Ejecutar pasos del modelo utilizando la Q-table cargada\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

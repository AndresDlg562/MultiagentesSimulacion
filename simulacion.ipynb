{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sembradora 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents an agent-based model that simulates the propagation of a disease through a network.\n",
    "It demonstrates how to use the [agentpy](https://agentpy.readthedocs.io) package to create and visualize networks, use the interactive module, and perform different types of sensitivity analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model design\n",
    "import agentpy as ap\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from queue import PriorityQueue\n",
    "from itertools import count\n",
    "import math\n",
    "\n",
    "# Visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the model\n",
    "\n",
    "The agents of this model are people, which can be in one of the following three conditions: susceptible to the disease (S), infected (I), or recovered (R). The agents are connected to each other through a small-world network of peers. At every time-step, infected agents can infect their peers or recover from the disease based on random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1 is tractor\n",
    "2 is obstacle\n",
    "3 is target\n",
    "4 is seeds\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def is_connected(grid, free_positions):\n",
    "    \"\"\" Check if all free cells are connected using BFS \"\"\"\n",
    "    n = grid.shape[0]\n",
    "    visited = set()\n",
    "    queue = deque([free_positions.pop()])\n",
    "    visited.add(queue[0])\n",
    "\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    \n",
    "    connected_count = 1\n",
    "    free_count = len(free_positions)\n",
    "\n",
    "    while queue:\n",
    "        x, y = queue.popleft()\n",
    "        for dx, dy in directions:\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            if 0 <= new_x < n and 0 <= new_y < n and (new_x, new_y) in free_positions and (new_x, new_y) not in visited:\n",
    "                queue.append((new_x, new_y))\n",
    "                visited.add((new_x, new_y))\n",
    "                connected_count += 1\n",
    "        \n",
    "                \n",
    "    return connected_count > free_count\n",
    "\n",
    "def generate_grid(model, n, obstacles_count):\n",
    "    grid = ap.Grid(model, (n, n), track_empty=True)  # Create an agentpy Grid object\n",
    "    grid.add_field(\"occupied\", 0)  # Add a field to store obstacle information\n",
    "\n",
    "    # Generate obstacle positions\n",
    "    obstacle_positions = set()\n",
    "    while len(obstacle_positions) < obstacles_count:\n",
    "        x, y = random.randint(0, n-1), random.randint(0, n-1)\n",
    "        if (x, y) not in obstacle_positions and (x,y) not in model.p.seedsPositions:\n",
    "            obstacle_positions.add((x, y))\n",
    "\n",
    "    # Mark grid cells as obstacles\n",
    "    for pos in obstacle_positions:\n",
    "        grid[\"occupied\"][pos] = 1\n",
    "\n",
    "    # Identify free positions\n",
    "    free_positions = set()\n",
    "    for pos in grid.all:\n",
    "        if grid[\"occupied\"][pos] != 1:\n",
    "            free_positions.add(pos)\n",
    "\n",
    "    # Check if the free cells are connected\n",
    "    final_obstacles = set()\n",
    "    while not is_connected(grid, free_positions):\n",
    "        final_obstacles.clear()\n",
    "        grid = ap.Grid(model, (n, n), track_empty=True)\n",
    "        grid.add_field(\"occupied\", 0)\n",
    "        obstacle_positions = set()\n",
    "        while len(obstacle_positions) < obstacles_count:\n",
    "            x, y = random.randint(0, n-1), random.randint(0, n-1)\n",
    "            if (x, y) not in obstacle_positions and (x,y) not in model.p.seedsPositions:\n",
    "                obstacle_positions.add((x, y))\n",
    "        \n",
    "        free_positions = set()\n",
    "        for pos in grid.all:\n",
    "            if grid[\"occupied\"][pos] == 0:\n",
    "                free_positions.add(pos)\n",
    "        final_obstacles = obstacle_positions\n",
    "\n",
    "    for pos in obstacle_positions:\n",
    "        grid[\"occupied\"][pos] = 1\n",
    "        model.np_grid[pos] = 3\n",
    "    #add an agent to each obstacle position with type 2\n",
    "    #Make an agentlist ap.agentlist\n",
    "    #Add the agent to the grid\n",
    "    #Add the agent to the agentlist\n",
    "    agentlist = ap.AgentList(model, len(obstacle_positions))\n",
    "    agentlist.type = 2\n",
    "        \n",
    "    grid.add_agents(agentlist, obstacle_positions)\n",
    "    \n",
    "\n",
    "\n",
    "    model.grid = grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectingTractor(ap.Agent):\n",
    "    def setup(self, type=1, pos=(0, 0)):\n",
    "        self.collected = 0\n",
    "        self.targetIndex = 1\n",
    "        self.path = []\n",
    "        self.destroyed = False\n",
    "        self.condition = True\n",
    "        self.seeds = 0\n",
    "        self.type = type\n",
    "        \n",
    "        # Machine Learning\n",
    "        self.start = pos\n",
    "        self.q_table = np.zeros((self.p.grid_size, self.p.grid_size, 6))  # 6 acciones posibles\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.1  \n",
    "        self.pos = pos  # Posición inicial\n",
    "        \n",
    "    # Función para moverse arrriba\n",
    "    def move_up(self):\n",
    "        x, y = self.pos\n",
    "        if y < self.p.grid_size - 1:\n",
    "            self.model.grid.move_to(self, (x, y + 1))\n",
    "            self.pos = (x, y + 1)\n",
    "            accionado = True\n",
    "            return self.pos, accionado\n",
    "        else:\n",
    "            accionado = False\n",
    "            return self.pos, accionado\n",
    "        \n",
    "    # Función para moverse abajo\n",
    "    def move_down(self):\n",
    "        x, y = self.pos\n",
    "        if y > 0:\n",
    "            self.model.grid.move_to(self, (x, y - 1))\n",
    "            self.pos = (x, y - 1)\n",
    "            accionado = True\n",
    "            return self.pos, accionado\n",
    "        else:\n",
    "            accionado = False\n",
    "            return self.pos, accionado\n",
    "        \n",
    "    # Función para moverse a la izquierda\n",
    "    def move_left(self):\n",
    "        x, y = self.pos\n",
    "        if x > 0:\n",
    "            self.model.grid.move_to(self, (x - 1, y))\n",
    "            self.pos = (x - 1, y)\n",
    "            accionado = True\n",
    "            return self.pos, accionado \n",
    "        else:\n",
    "            accionado = False\n",
    "            return self.pos, accionado\n",
    "        \n",
    "    # Función para moverse a la derecha\n",
    "    def move_right(self):\n",
    "        x, y = self.pos\n",
    "        if x < self.p.grid_size - 1:\n",
    "            self.model.grid.move_to(self, (x + 1, y))\n",
    "            self.pos = (x + 1, y)\n",
    "            accionado = True\n",
    "            return self.pos, accionado\n",
    "        else:\n",
    "            accionado = False\n",
    "            return self.pos, accionado\n",
    "    \n",
    "    # función para dejar una semilla\n",
    "    def drop(self):\n",
    "        x, y = self.pos\n",
    "        if self.seeds > 0 and self.pos == self.targets[self.targetIndex]:\n",
    "            self.targetIndex += 1\n",
    "            self.seeds -= 1\n",
    "            accionado = True\n",
    "            return self.pos, accionado\n",
    "        else:\n",
    "            accionado = False\n",
    "            return self.pos, accionado\n",
    "            \n",
    "    # función para recoger una semillas\n",
    "    def collect(self):\n",
    "        x, y = self.pos\n",
    "        if self.model.np_grid[self.pos] == 4 and self.seeds == 0: # If semillas == 0, agarra semilas\n",
    "            self.seeds = self.capacity\n",
    "            accionado = 1\n",
    "            return self.pos, accionado\n",
    "        elif self.model.np_grid[self.pos] == 4 and self.seeds < self.capacity: # If semillas < capacidad maxima, agarra semillas\n",
    "            self.seeds = self.capacity\n",
    "            accionado = 2\n",
    "            return self.pos, accionado\n",
    "        else: # Si semillas == capacidad maxima, no agarra semillas\n",
    "            accionado = 3\n",
    "            return self.pos, accionado\n",
    "    \n",
    "    \n",
    "    def get_reward(self, action, agents, accionado):\n",
    "        if action == 0:  # Arriba\n",
    "            if accionado == True:\n",
    "                reward = -5  # Penalización por step\n",
    "            else:\n",
    "                reward = -15  # Penalización por intentar salir del límite\n",
    "        elif action == 1:  # Abajo\n",
    "            if accionado == True:\n",
    "                reward = -5\n",
    "            else:\n",
    "                reward = -15\n",
    "        elif action == 2:  # Izquierda\n",
    "            if accionado == True:\n",
    "                reward = -5\n",
    "            else:\n",
    "                reward = -15\n",
    "        elif action == 3:  # Derecha\n",
    "            if accionado == True:\n",
    "                reward = -5\n",
    "            else:\n",
    "                reward = -15\n",
    "        elif action == 4:  # Dejar\n",
    "            if accionado == True:\n",
    "                reward = 10  # Recompensa positiva por dejar semillas\n",
    "            else:\n",
    "                reward = -15  # Recompensa negativa por intentar dejar sin semillas\n",
    "        elif action == 5:  # Recoger\n",
    "            if accionado == 1:\n",
    "                reward = -5\n",
    "            elif accionado == 2:\n",
    "                reward = -10\n",
    "            else:\n",
    "                reward = -15\n",
    "        else:\n",
    "            # Penalización pequeña por cada movimiento para incentivar la eficiencia\n",
    "            reward = -10\n",
    "\n",
    "        # Verificación de colisiones con otros agentes\n",
    "        for agent in agents:\n",
    "            if agent != self and agent.pos == self.pos:\n",
    "                reward -= 5  # Penalización por colisión con otro agente\n",
    "                break\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def q_learning_update(self, state, action, reward, next_state):\n",
    "        # Desempaqueta las coordenadas de estado y next_state\n",
    "        x, y = state\n",
    "        next_x, next_y = next_state\n",
    "\n",
    "        # Accede al valor Q actual\n",
    "        current_q = self.q_table[x, y, action]\n",
    "\n",
    "        # Encuentra el valor máximo de Q para el siguiente estado\n",
    "        max_next_q = np.max(self.q_table[next_x, next_y])\n",
    "\n",
    "        # Calcula el nuevo valor Q\n",
    "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
    "\n",
    "        # Actualiza la Q-table\n",
    "        self.q_table[x, y, action] = new_q\n",
    "\n",
    "\n",
    "    def acciones(self, action):\n",
    "        if action == 0:  # Arriba\n",
    "            return self.move_up()\n",
    "        elif action == 1:  # Abajo\n",
    "            return self.move_down()\n",
    "        elif action == 2:  # Izquierda\n",
    "            return self.move_left()\n",
    "        elif action == 3:  # Derecha\n",
    "            return self.move_right()\n",
    "        elif action == 4:  # Dejar\n",
    "            return self.drop()\n",
    "        elif action == 5:  # Recoger\n",
    "            return self.collect()\n",
    "\n",
    "\n",
    "    def step(self, agents):\n",
    "        state = self.pos  # Estado actual (x, y)\n",
    "        x, y = state\n",
    "        \n",
    "        # Selección de acción usando epsilon-greedy\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice([0, 1, 2, 3, 4, 5])  # Explorar\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[x, y])  # Explotar\n",
    "            \n",
    "        # Reduce epsilon (Exploración) con el tiempo\n",
    "        self.epsilon = max(0.1, self.epsilon * 0.995)  # Se asegura que epsilon no baje de 0.1\n",
    "\n",
    "        # Ejecuta la acción y obtiene el nuevo estado y el resultado\n",
    "        next_state, accionado = self.acciones(action)\n",
    "        next_x, next_y = next_state\n",
    "        \n",
    "        # Calcula la recompensa basada en la acción y el estado\n",
    "        reward = self.get_reward(action, agents, accionado)\n",
    "\n",
    "        # Actualiza la tabla Q\n",
    "        self.q_learning_update(state, action, reward, next_state)\n",
    "\n",
    "        # Actualiza la posición del agente\n",
    "        self.pos = next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TractorModel(ap.Model):\n",
    "    def setup(self):\n",
    "        self.Collected = 0\n",
    "        #Numpy array of size grid_size x grid_size\n",
    "        self.np_grid = np.zeros((self.p.grid_size, self.p.grid_size))\n",
    "        generate_grid(self, self.p.grid_size, self.p.obstacles_count)\n",
    "        #Create p.number_of_tractors tractors\n",
    "        self.agents = ap.AgentList(self, self.p.number_of_tractors, CollectingTractor)\n",
    "        self.agents.capacity = self.p.capacity\n",
    "        self.agents.seeds = self.p.starting_seeds\n",
    "        #Unique coords\n",
    "        coordsUsed = set()\n",
    "        #Set the targets for each tractor, checking that they are not obstacles\n",
    "        for tractor in self.agents:\n",
    "            targets = []\n",
    "            for i in range(self.p.number_of_targets + 1):\n",
    "                x, y = random.randint(0, self.p.grid_size-1), random.randint(0, self.p.grid_size-1)\n",
    "                while self.grid[\"occupied\"][(x, y)] == 1 or (x, y) in coordsUsed:\n",
    "                    x, y = random.randint(0, self.p.grid_size-1), random.randint(0, self.p.grid_size-1)\n",
    "                targets.append((x, y))\n",
    "                coordsUsed.add((x, y))\n",
    "            tractor.targets = targets\n",
    "            tractor.pos = targets[0]\n",
    "            self.np_grid[tractor.pos] = 1\n",
    "        \n",
    "        self.grid.add_agents(self.agents, [tractor.pos for tractor in self.agents])\n",
    "        \n",
    "    def update(self):\n",
    "        self.record('Collected', sum([tractor.collected for tractor in self.agents]))\n",
    "\n",
    "        \n",
    "    def step(self):\n",
    "        self.agents.step(self.agents)\n",
    "        #Assign 4 to the seed positions\n",
    "        for seed in self.p.seedsPositions:\n",
    "            self.np_grid[seed] = 4\n",
    "        \n",
    "    def end(self):\n",
    "        self.report('Total targets', self.p.number_of_targets * self.p.number_of_tractors)\n",
    "        #time to collect all targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 100 steps\n",
      "Run time: 0:00:00.026834\n",
      "Simulation finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tractorParameters = {\n",
    "    'grid_size': 10,\n",
    "    'obstacles_count': 10,\n",
    "    'number_of_tractors': 4,\n",
    "    'number_of_targets': 8,\n",
    "    'steps': 100,\n",
    "    'seedsPositions': [(0, 0)],\n",
    "    'capacity': 2,\n",
    "    'starting_seeds': 2,\n",
    "}\n",
    "model = TractorModel(tractorParameters)\n",
    "results = model.run(steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ejecutar un paso del modelo\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Acumular las recompensas obtenidas por todos los agentes\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39magents:\n",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m, in \u001b[0;36mTractorModel.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m#Assign 4 to the seed positions\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp\u001b[38;5;241m.\u001b[39mseedsPositions:\n",
      "File \u001b[1;32mc:\\Users\\ernes\\miniconda3\\envs\\tractores\\Lib\\site-packages\\agentpy\\sequences.py:102\u001b[0m, in \u001b[0;36mAttrIter.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AttrIter([\u001b[43mfunc_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m func_obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m])\n",
      "Cell \u001b[1;32mIn[3], line 183\u001b[0m, in \u001b[0;36mCollectingTractor.step\u001b[1;34m(self, agents)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.995\u001b[39m)  \u001b[38;5;66;03m# Se asegura que epsilon no baje de 0.1\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Ejecuta la acción y obtiene el nuevo estado y el resultado\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m next_state, accionado \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macciones\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m next_x, next_y \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Calcula la recompensa basada en la acción y el estado\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 164\u001b[0m, in \u001b[0;36mCollectingTractor.acciones\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmove_right()\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:  \u001b[38;5;66;03m# Dejar\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m:  \u001b[38;5;66;03m# Recoger\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect()\n",
      "Cell \u001b[1;32mIn[3], line 70\u001b[0m, in \u001b[0;36mCollectingTractor.drop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     69\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseeds \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargetIndex\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargetIndex \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseeds \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Configuración de parámetros\n",
    "num_episodes = 1000  # Número de episodios para entrenar y evaluar\n",
    "rewards_per_episode = []\n",
    "epsilon_values = []\n",
    "\n",
    "# Entrenamiento del agente\n",
    "for episode in range(num_episodes):\n",
    "    model = TractorModel(tractorParameters)  # Crear una nueva instancia del modelo para cada episodio\n",
    "    model.setup()  # Configurar el modelo\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        model.step()  # Ejecutar un paso del modelo\n",
    "        \n",
    "        # Acumular las recompensas obtenidas por todos los agentes\n",
    "        for agent in model.agents:\n",
    "            x, y = agent.pos\n",
    "            action = np.argmax(agent.q_table[x, y])  # Obtener la acción elegida\n",
    "            accionado = agent.acciones(action)[1]\n",
    "            reward = agent.get_reward(action, model.agents, accionado)  # Obtener recompensa\n",
    "            total_reward += reward  # Acumular la recompensa obtenida\n",
    "        \n",
    "        if model.end():  # Verificar si se cumple la condición de finalización del episodio\n",
    "            break\n",
    "\n",
    "    # Guardar la recompensa total del episodio y el valor de epsilon\n",
    "    rewards_per_episode.append(total_reward)\n",
    "    epsilon_values.append(model.agents[0].epsilon)  # Se asume que todos los agentes tienen el mismo epsilon\n",
    "    \n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}, Epsilon = {model.agents[0].epsilon}\")\n",
    "\n",
    "# Graficar el rendimiento\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Recompensa acumulada por episodio\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_per_episode, label='Total Reward per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Reward per Episode')\n",
    "plt.legend()\n",
    "\n",
    "# Evolución de epsilon\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epsilon_values, label='Epsilon Value')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.title('Epsilon Decay over Episodes')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "ESTO NO JALA, PERO SI ALGUIEN LO QUIERE ARREGLAR, DESE\n",
    "def tractor_plot(data, ax):\n",
    "    x = data.index.get_level_values('t')\n",
    "    y = data['Collected']\n",
    "    ax.plot(x, y, label='Collected targets')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(0, max(1, len(x)-1))\n",
    "    ax.set_ylim(0, 25)\n",
    "    ax.set_xlabel(\"Time steps\")\n",
    "    ax.set_ylabel(\"Number of collected targets\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "tractor_plot(results.variables.TractorModel, ax)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "0 is empty\n",
    "1 is tractor\n",
    "2 is obstacle\n",
    "3 is target\n",
    "4 is seeds\n",
    "\"\"\"\n",
    "\n",
    "# Load images\n",
    "tractor_img = mpimg.imread('tractor.png')\n",
    "obstacle_img = mpimg.imread('obstacle.png')\n",
    "target_img = mpimg.imread('target.png')\n",
    "seeds_img = mpimg.imread('seeds.png')\n",
    "\n",
    "def animation_plot(model, ax):\n",
    "    # Clear the axis to avoid over-plotting\n",
    "    ax.clear()\n",
    "\n",
    "    # Plot the grid using images\n",
    "    for (x, y), value in np.ndenumerate(model.np_grid):\n",
    "        if value == 1:  # Tractor\n",
    "            ax.imshow(tractor_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "        elif value == 3:  # Obstacle\n",
    "            ax.imshow(obstacle_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "        elif value == 2:  # Target\n",
    "            ax.imshow(target_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "        elif value == 4:  # Seeds\n",
    "            ax.imshow(seeds_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "\n",
    "    # Add text for each tractor displaying the number of seeds it has\n",
    "    for agent in model.agents:\n",
    "        if agent.destroyed:\n",
    "            continue\n",
    "        ax.text(agent.pos[1] + 0.5, agent.pos[0] + 0.5, str(agent.seeds),\n",
    "                color='black', fontsize=12, ha='center', va='center', weight='bold')\n",
    "\n",
    "    # Fix axis limits based on the grid dimensions\n",
    "    ax.set_xlim([0, model.np_grid.shape[1]])\n",
    "    ax.set_ylim([0, model.np_grid.shape[0]])\n",
    "\n",
    "    # Set aspect ratio to 'equal' to prevent image stretching\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Set the title for the plot\n",
    "    ax.set_title(f\"Tractor model \\n Time-step: {model.t}, Collected: {model.Collected}\")\n",
    "\n",
    "# Example usage\n",
    "fig, ax = plt.subplots()\n",
    "model = TractorModel(tractorParameters)\n",
    "animation = ap.animate(model, fig, ax, animation_plot)\n",
    "animation.save('simulacionTractores.gif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the animation\n",
    "HTML(animation.to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sembradora 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents an agent-based model that simulates the propagation of a disease through a network.\n",
    "It demonstrates how to use the [agentpy](https://agentpy.readthedocs.io) package to create and visualize networks, use the interactive module, and perform different types of sensitivity analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model design\n",
    "import agentpy as ap\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from queue import PriorityQueue\n",
    "from itertools import count\n",
    "import math\n",
    "\n",
    "# Visualization\n",
    "import imageio\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Guardar\n",
    "import pickle\n",
    "\n",
    "# Sistema\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the model\n",
    "\n",
    "The agents of this model are people, which can be in one of the following three conditions: susceptible to the disease (S), infected (I), or recovered (R). The agents are connected to each other through a small-world network of peers. At every time-step, infected agents can infect their peers or recover from the disease based on random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1 is tractor\n",
    "2 is obstacle\n",
    "3 is target\n",
    "4 is seeds\n",
    "\"\"\"\n",
    "\n",
    "def is_connected(grid, free_positions):\n",
    "    \"\"\"Check if all free cells are connected using BFS.\"\"\"\n",
    "    n = grid.shape[0]\n",
    "    visited = set()\n",
    "    queue = deque([free_positions.pop()])\n",
    "    visited.add(queue[0])\n",
    "\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    \n",
    "    connected_count = 1\n",
    "    free_count = len(free_positions)\n",
    "\n",
    "    while queue:\n",
    "        x, y = queue.popleft()\n",
    "        for dx, dy in directions:\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            if (0 <= new_x < n and 0 <= new_y < n and \n",
    "                (new_x, new_y) in free_positions and (new_x, new_y) not in visited):\n",
    "                queue.append((new_x, new_y))\n",
    "                visited.add((new_x, new_y))\n",
    "                connected_count += 1\n",
    "                \n",
    "    return connected_count == free_count\n",
    "\n",
    "def generate_grid(model, n, obstacles_count):\n",
    "    \"\"\"Generate a grid with obstacles, ensuring free cells are connected.\"\"\"\n",
    "    while True:\n",
    "        grid = ap.Grid(model, (n, n), track_empty=True)\n",
    "        grid.add_field(\"occupied\", 0)\n",
    "\n",
    "        obstacle_positions = set()\n",
    "        while len(obstacle_positions) < obstacles_count:\n",
    "            pos = (random.randint(0, n-1), random.randint(0, n-1))\n",
    "            if pos not in obstacle_positions and pos not in model.p.seedsPositions:\n",
    "                obstacle_positions.add(pos)\n",
    "\n",
    "        for pos in obstacle_positions:\n",
    "            grid[\"occupied\"][pos] = 1\n",
    "\n",
    "        free_positions = {(x, y) for x in range(n) for y in range(n) if grid[\"occupied\"][x, y] == 0}\n",
    "        \n",
    "        if is_connected(grid, free_positions):\n",
    "            break\n",
    "\n",
    "    for pos in obstacle_positions:\n",
    "        grid[\"occupied\"][pos] = 1\n",
    "        model.np_grid[pos] = 3\n",
    "\n",
    "    agentlist = ap.AgentList(model, len(obstacle_positions), agent_type=2)\n",
    "    grid.add_agents(agentlist, obstacle_positions)\n",
    "    \n",
    "    model.grid = grid\n",
    "    \n",
    "    return obstacle_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectingTractor(ap.Agent):\n",
    "    def setup(self):\n",
    "        \"\"\"Initialize agent parameters.\"\"\"\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset agent attributes.\"\"\"\n",
    "        self.collected = 0\n",
    "        self.targetIndex = 1\n",
    "        self.destroyed = False\n",
    "        self.condition = True\n",
    "        self.seeds = 0\n",
    "        self.type = 1\n",
    "        self.pos = (0, 0)\n",
    "\n",
    "        # Initialize Q-learning parameters\n",
    "        self.q_table = np.zeros((self.p.grid_size, self.p.grid_size, 6))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.0\n",
    "        self.epsilon_decay = 0.990\n",
    "\n",
    "    def move(self, dx=0, dy=0):\n",
    "        \"\"\"Move the tractor by (dx, dy).\"\"\"\n",
    "        x, y = self.pos\n",
    "        new_pos = (x + dx, y + dy)\n",
    "\n",
    "        if 0 <= new_pos[0] < self.p.grid_size and 0 <= new_pos[1] < self.p.grid_size:\n",
    "            self.model.grid.move_to(self, new_pos)\n",
    "            self.pos = new_pos\n",
    "            return new_pos, True\n",
    "        return self.pos, False\n",
    "\n",
    "    def move_up(self):\n",
    "        return self.move(dy=1)\n",
    "\n",
    "    def move_down(self):\n",
    "        return self.move(dy=-1)\n",
    "\n",
    "    def move_left(self):\n",
    "        return self.move(dx=-1)\n",
    "\n",
    "    def move_right(self):\n",
    "        return self.move(dx=1)\n",
    "\n",
    "    def drop(self):\n",
    "        \"\"\"Drop seeds at the current target position.\"\"\"\n",
    "        if self.seeds > 0 and self.pos == self.targets[self.targetIndex]:\n",
    "            self.targetIndex += 1\n",
    "            self.seeds -= 1\n",
    "            self.destroyed = self.targetIndex >= len(self.targets)\n",
    "            return self.pos, True\n",
    "        return self.pos, False\n",
    "\n",
    "    def collect(self):\n",
    "        \"\"\"Collect seeds if available at the current position.\"\"\"\n",
    "        if self.model.np_grid[self.pos] == 4:\n",
    "            self.seeds = self.capacity\n",
    "            return self.pos, 1 if self.seeds == 0 else 2\n",
    "        return self.pos, 3\n",
    "\n",
    "    def get_reward(self, action, agents, accionado):\n",
    "        \"\"\"Calculate the reward based on the action and outcome.\"\"\"\n",
    "        reward = -1  # Base penalty for movement\n",
    "        \n",
    "        # Calculate the distance to the current target\n",
    "        target_pos = self.targets[self.targetIndex]\n",
    "        distance_to_target = np.linalg.norm(np.array(self.pos) - np.array(target_pos))\n",
    "        \n",
    "        # Reduce penalty based on proximity to the target\n",
    "        proximity_bonus = max(0, (self.p.grid_size - distance_to_target) / self.p.grid_size)\n",
    "        \n",
    "        if action in [0, 1, 2, 3]:  # Movement actions\n",
    "            reward += -2 if not accionado else 0\n",
    "        elif action == 4:  # Drop seeds\n",
    "            reward = 100 if accionado else -15\n",
    "        elif action == 5:  # Collect seeds\n",
    "            reward = {1: 50, 2: -2, 3: -5}[accionado]\n",
    "        \n",
    "        # Add the proximity bonus to reward\n",
    "        reward += proximity_bonus\n",
    "        \n",
    "        # Penalty for collision with other agents\n",
    "        if any(agent != self and agent.pos == self.pos for agent in agents):\n",
    "            reward -= 5\n",
    "\n",
    "        # Penalty for collision with obstacles\n",
    "        if self.model.np_grid[self.pos] == 2:\n",
    "            reward -= 7\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def q_learning_update(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-table using the Q-learning algorithm.\"\"\"\n",
    "        x, y = state\n",
    "        next_x, next_y = next_state\n",
    "        current_q = self.q_table[x, y, action]\n",
    "        max_next_q = np.max(self.q_table[next_x, next_y])\n",
    "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
    "        self.q_table[x, y, action] = new_q\n",
    "\n",
    "    def acciones(self, action):\n",
    "        \"\"\"Map action to the corresponding method.\"\"\"\n",
    "        action_map = {\n",
    "            0: self.move_up,\n",
    "            1: self.move_down,\n",
    "            2: self.move_left,\n",
    "            3: self.move_right,\n",
    "            4: self.drop,\n",
    "            5: self.collect\n",
    "        }\n",
    "        return action_map[action]()\n",
    "\n",
    "    def step(self, agents):\n",
    "        \"\"\"Execute a step in the agent's behavior.\"\"\"\n",
    "        if self.destroyed:\n",
    "            return\n",
    "\n",
    "        state = self.pos\n",
    "        x, y = state\n",
    "        \n",
    "        # Action selection using epsilon-greedy\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice([0, 1, 2, 3, 4, 5])\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[x, y])\n",
    "        \n",
    "        # Decay epsilon to reduce exploration over time\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        # Execute the action and update Q-table\n",
    "        next_state, accionado = self.acciones(action)\n",
    "        reward = self.get_reward(action, agents, accionado)\n",
    "        self.q_learning_update(state, action, reward, next_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TractorModel(ap.Model):\n",
    "    def setup(self):\n",
    "        \"\"\"Initialize the environment and generate coordinates.\"\"\"\n",
    "        self.initialized = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset or initialize the model state.\"\"\"\n",
    "        if not self.initialized:\n",
    "            self._initialize_environment()\n",
    "        else:\n",
    "            self._restore_environment()\n",
    "        \n",
    "        self.steps = 0  # Reset the step counter\n",
    "\n",
    "    def _initialize_environment(self):\n",
    "        \"\"\"Initialize the environment, agents, and their targets.\"\"\"\n",
    "        self.Collected = 0\n",
    "        self.steps = 0\n",
    "        self.max_steps = self.p.max_steps\n",
    "        self.np_grid = np.zeros((self.p.grid_size, self.p.grid_size))\n",
    "\n",
    "        # Generate and store obstacle positions\n",
    "        occupied_positions = generate_grid(self, self.p.grid_size, self.p.obstacles_count)\n",
    "        self.obstacle_positions = np.copy(self.grid[\"occupied\"])\n",
    "\n",
    "        # Create agents and assign initial properties\n",
    "        self.agents = ap.AgentList(self, self.p.number_of_tractors, CollectingTractor)\n",
    "        self._initialize_agents(occupied_positions)\n",
    "\n",
    "        self.grid.add_agents(self.agents, [tractor.pos for tractor in self.agents])\n",
    "        self.initialized = True  # Mark as initialized\n",
    "\n",
    "    def _restore_environment(self):\n",
    "        \"\"\"Restore the environment to its initial state.\"\"\"\n",
    "        self.grid[\"occupied\"] = np.copy(self.obstacle_positions)\n",
    "        self.np_grid = np.zeros((self.p.grid_size, self.p.grid_size))\n",
    "\n",
    "        for tractor, targets in zip(self.agents, self.targets_by_tractor):\n",
    "            tractor.reset()  # Reset each agent\n",
    "            tractor.targets = targets\n",
    "            tractor.pos = targets[0]\n",
    "            self.np_grid[tractor.pos] = 1\n",
    "\n",
    "        self.grid.add_agents(self.agents, [tractor.pos for tractor in self.agents])\n",
    "\n",
    "    def _initialize_agents(self, occupied_positions):\n",
    "        \"\"\"Initialize agents with positions and targets.\"\"\"\n",
    "        self.agents.capacity = self.p.capacity\n",
    "        self.agents.seeds = self.p.starting_seeds\n",
    "        self.agents.targetIndex = 1\n",
    "        self.agents.destroyed = False\n",
    "\n",
    "        self.coordsUsed = set(occupied_positions)\n",
    "        self.targets_by_tractor = []\n",
    "\n",
    "        for tractor in self.agents:\n",
    "            targets = self._generate_targets_for_tractor()\n",
    "            self.targets_by_tractor.append(targets)\n",
    "            tractor.targets = targets\n",
    "            tractor.pos = targets[0]\n",
    "            self.np_grid[tractor.pos] = 1\n",
    "\n",
    "    def _generate_targets_for_tractor(self):\n",
    "        \"\"\"Generate target positions for a tractor.\"\"\"\n",
    "        targets = []\n",
    "        for _ in range(self.p.number_of_targets + 1):\n",
    "            pos = self._get_free_position()\n",
    "            targets.append(pos)\n",
    "            self.coordsUsed.add(pos)\n",
    "        return targets\n",
    "\n",
    "    def _get_free_position(self):\n",
    "        \"\"\"Get a free position on the grid.\"\"\"\n",
    "        while True:\n",
    "            x, y = random.randint(0, self.p.grid_size - 1), random.randint(0, self.p.grid_size - 1)\n",
    "            if self.grid[\"occupied\"][(x, y)] == 0 and (x, y) not in self.coordsUsed:\n",
    "                return (x, y)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Execute a step in the simulation.\"\"\"\n",
    "        for seed in self.p.seedsPositions:\n",
    "            self.np_grid[seed] = 4\n",
    "\n",
    "        self.steps += 1\n",
    "        self.agents.step(self.agents)\n",
    "\n",
    "        if self.steps >= self.max_steps:\n",
    "            for tractor in self.agents:\n",
    "                tractor.destroyed = True\n",
    "\n",
    "    def end(self):\n",
    "        \"\"\"Report the total number of targets at the end of the simulation.\"\"\"\n",
    "        self.report('Total targets', self.p.number_of_targets * self.p.number_of_tractors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "0 is empty\n",
    "1 is tractor\n",
    "2 is obstacle\n",
    "3 is target\n",
    "4 is seeds\n",
    "\"\"\"\n",
    "\n",
    "# Load images\n",
    "tractor_img = mpimg.imread('tractor.png')\n",
    "obstacle_img = mpimg.imread('obstacle.png')\n",
    "target_img = mpimg.imread('target.png')\n",
    "seeds_img = mpimg.imread('seeds.png')\n",
    "\n",
    "def save_frame(model, filename):\n",
    "    fig, ax = plt.subplots()\n",
    "    # Clear the axis to avoid over-plotting\n",
    "    ax.clear()\n",
    "\n",
    "    # Plot the grid using images\n",
    "    for (x, y), value in np.ndenumerate(model.np_grid):\n",
    "        if value == 1:  # Tractor\n",
    "            ax.imshow(tractor_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "        elif value == 2:  # Obstacle\n",
    "            ax.imshow(obstacle_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "        elif value == 3:  # Target\n",
    "            ax.imshow(target_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "        elif value == 4:  # Seeds\n",
    "            ax.imshow(seeds_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "\n",
    "    # Add text for each tractor displaying the number of seeds it has\n",
    "    for agent in model.agents:\n",
    "        if agent.destroyed:\n",
    "            continue\n",
    "        ax.text(agent.pos[1] + 0.5, agent.pos[0] + 0.5, str(agent.seeds),\n",
    "                color='black', fontsize=12, ha='center', va='center', weight='bold')\n",
    "\n",
    "    # Fix axis limits based on the grid dimensions\n",
    "    ax.set_xlim([0, model.np_grid.shape[1]])\n",
    "    ax.set_ylim([0, model.np_grid.shape[0]])\n",
    "\n",
    "    # Set aspect ratio to 'equal' to prevent image stretching\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Set the title for the plot\n",
    "    ax.set_title(f\"Tractor model \\n Time-step: {model.steps}, Collected: {model.Collected}\")\n",
    "\n",
    "    # Save the frame\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def create_gif(filenames, gif_filename):\n",
    "    with imageio.get_writer(gif_filename, mode='I', duration=0.1) as writer:\n",
    "        for filename in filenames:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "    \n",
    "def delete_temp_frames(prefix):\n",
    "    for filename in os.listdir():\n",
    "        if prefix in filename:\n",
    "            os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tractorParameters = {\n",
    "    'grid_size': 10,\n",
    "    'obstacles_count': 10,\n",
    "    'number_of_tractors': 2,\n",
    "    'number_of_targets': 20,\n",
    "    'max_steps': 100,\n",
    "    'seedsPositions': [(0, 0)],\n",
    "    'capacity': 2,\n",
    "    'starting_seeds': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aprendizaje:\n",
    "num_episodes = 1001\n",
    "rewards_per_episode = []\n",
    "epsilon_values = []\n",
    "\n",
    "model = TractorModel(tractorParameters)\n",
    "model.setup()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    model.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "\n",
    "    # Primer episodio: sin exploración \n",
    "    if episode == 0:  \n",
    "        # Store frames for the final episode\n",
    "        filenames = []\n",
    "        while model.steps < model.max_steps:\n",
    "            filename = f'frame_{model.steps}.png'\n",
    "            save_frame(model, filename)\n",
    "            filenames.append(filename)\n",
    "            model.step()  # Ejecutar un paso en el modelo\n",
    "            total_reward += sum([tractor.get_reward(0, model.agents, False) for tractor in model.agents])\n",
    "\n",
    "            # Verificar si todos los tractores están destruidos y romper el loop\n",
    "            if all([tractor.destroyed for tractor in model.agents]):\n",
    "                break\n",
    "\n",
    "        # Create the GIF from saved frames\n",
    "        create_gif(filenames, '1rstSimulacionTractores.gif')\n",
    "\n",
    "        # Delete temporary frame files\n",
    "        delete_temp_frames('frame')\n",
    "        \n",
    "    # Último episodio: \n",
    "    if episode == num_episodes - 1:  \n",
    "        for tractor in model.agents:\n",
    "            tractor.epsilon = 0.0\n",
    "        \n",
    "        # Store frames for the final episode\n",
    "        filenames = []\n",
    "        while model.steps < model.max_steps:\n",
    "            filename = f'frame_{model.steps}.png'\n",
    "            save_frame(model, filename)\n",
    "            filenames.append(filename)\n",
    "            model.step()  # Ejecutar un paso en el modelo\n",
    "            total_reward += sum([tractor.get_reward(0, model.agents, False) for tractor in model.agents])\n",
    "\n",
    "            # Verificar si todos los tractores están destruidos y romper el loop\n",
    "            if all([tractor.destroyed for tractor in model.agents]):\n",
    "                break\n",
    "\n",
    "        # Create the GIF from saved frames\n",
    "        create_gif(filenames, 'lastSimulacionTractores.gif')\n",
    "\n",
    "        # Delete temporary frame files\n",
    "        delete_temp_frames('frame')\n",
    "\n",
    "    # Loop para los pasos dentro del episodio\n",
    "    while model.steps < model.max_steps:\n",
    "        model.step()  # Ejecutar un paso en el modelo\n",
    "        total_reward += sum([tractor.get_reward(0, model.agents, False) for tractor in model.agents])\n",
    "\n",
    "        # Verificar si todos los tractores están destruidos y romper el loop\n",
    "        if all([tractor.destroyed for tractor in model.agents]):\n",
    "            break\n",
    "\n",
    "    rewards_per_episode.append(total_reward)\n",
    "    epsilon_values.append(model.agents[0].epsilon)\n",
    "\n",
    "    # Monitorear el progreso cada 100 episodios\n",
    "    if episode % 10 == 0:\n",
    "        print(f'Episodio {episode + 1}/{num_episodes}, Recompensa total: {total_reward:.2f}, Epsilon: {model.agents[0].epsilon:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar la Q-table al final del entrenamiento\n",
    "q_table = model.agents[0].q_table  # Asumiendo que todos los agentes comparten la misma Q-table\n",
    "with open('q_table.pkl', 'wb') as f:\n",
    "    pickle.dump(q_table, f)\n",
    "\n",
    "# Graficar el rendimiento\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_per_episode, label='Total Reward per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Reward per Episode')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Asignar la Q-table cargada a los agentes\n",
    "model = TractorModel(tractorParameters)\n",
    "model.setup()\n",
    "\n",
    "for agent in model.agents:\n",
    "    agent.q_table = q_table  # Asignar la Q-table cargada a cada agente\n",
    "\n",
    "# Ahora puedes continuar entrenando o simular el comportamiento\n",
    "model.reset()  # Resetear el ambiente si es necesario\n",
    "model.step()  # Ejecutar pasos del modelo utilizando la Q-table cargada\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sembradora 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents an agent-based model that simulates the propagation of a disease through a network.\n",
    "It demonstrates how to use the [agentpy](https://agentpy.readthedocs.io) package to create and visualize networks, use the interactive module, and perform different types of sensitivity analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model design\n",
    "import agentpy as ap\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from queue import PriorityQueue\n",
    "from itertools import count\n",
    "import math\n",
    "\n",
    "# Visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Machine Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the model\n",
    "\n",
    "The agents of this model are people, which can be in one of the following three conditions: susceptible to the disease (S), infected (I), or recovered (R). The agents are connected to each other through a small-world network of peers. At every time-step, infected agents can infect their peers or recover from the disease based on random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectingTractor(ap.Agent):\n",
    "    def setup(self, type = 1, pos = (0,0)):\n",
    "        self.collected = 0\n",
    "        self.targetIndex = 1\n",
    "        self.path = []\n",
    "        self.destroyed = False\n",
    "        self.condition = True\n",
    "        self.seeds = 0\n",
    "        self.type = type\n",
    "        self.pos = pos\n",
    "        self.start = pos\n",
    "\n",
    "\n",
    "    def heuristic(x1, y1, x2, y2):\n",
    "        return abs(x1 - x2) + abs(y1 - y2)\n",
    "    \n",
    "    def a_star_search(self, start, end):\n",
    "        #initialize the frontier using the initial state of the problem\n",
    "        frontier = PriorityQueue()\n",
    "        frontier.put(start, 0)\n",
    "        came_from = {}\n",
    "        cost_so_far = {}\n",
    "        came_from[start] = None\n",
    "        cost_so_far[start] = 0\n",
    "\n",
    "        #run the search\n",
    "        while not frontier.empty():\n",
    "            current = frontier.get()\n",
    "\n",
    "            if current == end:\n",
    "                break\n",
    "\n",
    "            #add to negihbors all the neighbors of the current cell\n",
    "            neighbors = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "            size = self.model.grid.shape[0]\n",
    "\n",
    "            for nextTemp in neighbors:\n",
    "                #check if the next cell is within the grid\n",
    "                next = (current[0]+ nextTemp[0], current[1] + nextTemp[1])\n",
    "                if next[0] < 0 or next[0] >= size or next[1] < 0 or next[1] >= size:\n",
    "                    continue\n",
    "                #check if the next cell is an obstacle\n",
    "                if self.model.grid[\"occupied\"][next] == 1:\n",
    "                    continue\n",
    "\n",
    "                new_cost = cost_so_far[current] + 1\n",
    "                if next not in cost_so_far or new_cost < cost_so_far[next]:\n",
    "                    cost_so_far[next] = new_cost\n",
    "                    priority = new_cost + abs(end[0] - next[0]) + abs(end[1] - next[1])\n",
    "                    frontier.put(next, priority)\n",
    "                    came_from[next] = current\n",
    "\n",
    "        #reconstruct the path\n",
    "        path = []\n",
    "        current = end\n",
    "        while current != start:\n",
    "            path.append(current)\n",
    "            current = came_from[current]\n",
    "        path.reverse()\n",
    "        return path\n",
    "        \n",
    "    def collect(self):\n",
    "        #run an A* search to find the shortest path to the target\n",
    "        if (self.type == 2):\n",
    "            return\n",
    "\n",
    "        if (self.destroyed):\n",
    "            return\n",
    "\n",
    "\n",
    "        start = (self.pos[0], self.pos[1])\n",
    "        if (self.seeds > 0):\n",
    "            end = self.targets[self.targetIndex]\n",
    "        else:\n",
    "            #iterate through the seeds and find the closest one using A*\n",
    "            minDist = 100000\n",
    "            for seed in self.model.p.seedsPositions:\n",
    "                dist = len(self.a_star_search(start, seed))\n",
    "                if (dist < minDist):\n",
    "                    minDist = dist\n",
    "                    end = seed\n",
    "\n",
    "        self.model.np_grid[end] = 2\n",
    "        if (len(self.path) == 0):\n",
    "            self.path = self.a_star_search(start, end)\n",
    "\n",
    "        \n",
    "        if (len(self.path) == 0):\n",
    "            next_pos = self.pos\n",
    "        else:\n",
    "            next_pos = self.path.pop(0)\n",
    "            if not (next_pos in self.model.grid.empty.items):\n",
    "                return        \n",
    "        \n",
    "        self.model.grid.move_to(self, next_pos)\n",
    "        self.model.np_grid[self.pos] = 0\n",
    "        self.model.np_grid[next_pos] = 1\n",
    "        self.pos = next_pos\n",
    "\n",
    "        if (self.pos in self.model.p.seedsPositions):\n",
    "            self.seeds = self.capacity\n",
    "            return\n",
    "        #check if the target has been reached\n",
    "\n",
    "        if self.pos == end:\n",
    "            self.seeds -= 1\n",
    "            self.model.Collected += 1\n",
    "            self.targetIndex += 1\n",
    "            if self.targetIndex == len(self.targets):\n",
    "                self.model.np_grid[self.pos] = 0\n",
    "                self.model.grid.remove_agents(self)\n",
    "                self.destroyed = True\n",
    "                self.condition = False\n",
    "                self.type = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definir grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1 is tractor\n",
    "2 is obstacle\n",
    "3 is target\n",
    "4 is seeds\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def is_connected(grid, free_positions):\n",
    "    \"\"\" Check if all free cells are connected using BFS \"\"\"\n",
    "    n = grid.shape[0]\n",
    "    visited = set()\n",
    "    queue = deque([free_positions.pop()])\n",
    "    visited.add(queue[0])\n",
    "\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    \n",
    "    connected_count = 1\n",
    "    free_count = len(free_positions)\n",
    "\n",
    "    while queue:\n",
    "        x, y = queue.popleft()\n",
    "        for dx, dy in directions:\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            if 0 <= new_x < n and 0 <= new_y < n and (new_x, new_y) in free_positions and (new_x, new_y) not in visited:\n",
    "                queue.append((new_x, new_y))\n",
    "                visited.add((new_x, new_y))\n",
    "                connected_count += 1\n",
    "        \n",
    "                \n",
    "    return connected_count > free_count\n",
    "\n",
    "def generate_grid(model, n, obstacles_count):\n",
    "    grid = ap.Grid(model, (n, n), track_empty=True)  # Create an agentpy Grid object\n",
    "    grid.add_field(\"occupied\", 0)  # Add a field to store obstacle information\n",
    "\n",
    "    # Generate obstacle positions\n",
    "    obstacle_positions = set()\n",
    "    while len(obstacle_positions) < obstacles_count:\n",
    "        x, y = random.randint(0, n-1), random.randint(0, n-1)\n",
    "        if (x, y) not in obstacle_positions and (x,y) not in model.p.seedsPositions:\n",
    "            obstacle_positions.add((x, y))\n",
    "\n",
    "    # Mark grid cells as obstacles\n",
    "    for pos in obstacle_positions:\n",
    "        grid[\"occupied\"][pos] = 1\n",
    "\n",
    "    # Identify free positions\n",
    "    free_positions = set()\n",
    "    for pos in grid.all:\n",
    "        if grid[\"occupied\"][pos] != 1:\n",
    "            free_positions.add(pos)\n",
    "\n",
    "    # Check if the free cells are connected\n",
    "    final_obstacles = set()\n",
    "    while not is_connected(grid, free_positions):\n",
    "        final_obstacles.clear()\n",
    "        grid = ap.Grid(model, (n, n), track_empty=True)\n",
    "        grid.add_field(\"occupied\", 0)\n",
    "        obstacle_positions = set()\n",
    "        while len(obstacle_positions) < obstacles_count:\n",
    "            x, y = random.randint(0, n-1), random.randint(0, n-1)\n",
    "            if (x, y) not in obstacle_positions and (x,y) not in model.p.seedsPositions:\n",
    "                obstacle_positions.add((x, y))\n",
    "        \n",
    "        free_positions = set()\n",
    "        for pos in grid.all:\n",
    "            if grid[\"occupied\"][pos] == 0:\n",
    "                free_positions.add(pos)\n",
    "        final_obstacles = obstacle_positions\n",
    "\n",
    "    for pos in obstacle_positions:\n",
    "        grid[\"occupied\"][pos] = 1\n",
    "        model.np_grid[pos] = 3\n",
    "    #add an agent to each obstacle position with type 2\n",
    "    #Make an agentlist ap.agentlist\n",
    "    #Add the agent to the grid\n",
    "    #Add the agent to the agentlist\n",
    "    agentlist = ap.AgentList(model, len(obstacle_positions))\n",
    "    agentlist.type = 2\n",
    "        \n",
    "    grid.add_agents(agentlist, obstacle_positions)\n",
    "    \n",
    "\n",
    "\n",
    "    model.grid = grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TractorModel(ap.Model):\n",
    "    def setup(self):\n",
    "        self.Collected = 0\n",
    "        #numpy array of size grid_size x grid_size\n",
    "        self.np_grid = np.zeros((self.p.grid_size, self.p.grid_size))\n",
    "        generate_grid(self, self.p.grid_size, self.p.obstacles_count)\n",
    "        #Create p.number_of_tractors tractors\n",
    "        self.agents = ap.AgentList(self, self.p.number_of_tractors, CollectingTractor)\n",
    "        self.agents.capacity = self.p.capacity\n",
    "        self.agents.seeds = self.p.starting_seeds\n",
    "        #Unique coords\n",
    "        coordsUsed = set()\n",
    "        #Set the targets for each tractor, checking that they are not obstacles\n",
    "        for tractor in self.agents:\n",
    "            targets = []\n",
    "            for i in range(self.p.number_of_targets + 1):\n",
    "                x, y = random.randint(0, self.p.grid_size-1), random.randint(0, self.p.grid_size-1)\n",
    "                while self.grid[\"occupied\"][(x, y)] == 1 or (x, y) in coordsUsed:\n",
    "                    x, y = random.randint(0, self.p.grid_size-1), random.randint(0, self.p.grid_size-1)\n",
    "                targets.append((x, y))\n",
    "                coordsUsed.add((x, y))\n",
    "            tractor.targets = targets\n",
    "            tractor.pos = targets[0]\n",
    "            self.np_grid[tractor.pos] = 1\n",
    "        \n",
    "        self.grid.add_agents(self.agents, [tractor.pos for tractor in self.agents])\n",
    "        \n",
    "    def update(self):\n",
    "        self.record('Collected', sum([tractor.collected for tractor in self.agents]))\n",
    "\n",
    "        \n",
    "    def step(self):\n",
    "            self.agents.collect()\n",
    "            #Assign 4 to the seed positions\n",
    "            for seed in self.p.seedsPositions:\n",
    "                self.np_grid[seed] = 4\n",
    "        \n",
    "    def end(self):\n",
    "        self.report('Total targets', self.p.number_of_targets * self.p.number_of_tractors)\n",
    "        #time to collect all targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tractorParameters = {\n",
    "    'grid_size': 10,\n",
    "    'obstacles_count': 10,\n",
    "    'number_of_tractors': 4,\n",
    "    'number_of_targets': 8,\n",
    "    'steps': 100,\n",
    "    'seedsPositions': [(0, 0)],\n",
    "    'capacity': 2,\n",
    "    'starting_seeds': 2\n",
    "}\n",
    "\n",
    "model = TractorModel(tractorParameters)\n",
    "results = model.run(steps=100)\n",
    "'''model = VirusModel(parameters)\n",
    "results = model.run() '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = model.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = model.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "ESTO NO JALA, PERO SI ALGUIEN LO QUIERE ARREGLAR, DESE\n",
    "def tractor_plot(data, ax):\n",
    "    x = data.index.get_level_values('t')\n",
    "    y = data['Collected']\n",
    "    ax.plot(x, y, label='Collected targets')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(0, max(1, len(x)-1))\n",
    "    ax.set_ylim(0, 25)\n",
    "    ax.set_xlabel(\"Time steps\")\n",
    "    ax.set_ylabel(\"Number of collected targets\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "tractor_plot(results.variables.TractorModel, ax)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "0 is empty\n",
    "1 is tractor\n",
    "2 is obstacle\n",
    "3 is target\n",
    "4 is seeds\n",
    "\"\"\"\n",
    "\n",
    "# Load images\n",
    "tractor_img = mpimg.imread('tractor.png')\n",
    "obstacle_img = mpimg.imread('obstacle.png')\n",
    "target_img = mpimg.imread('target.png')\n",
    "seeds_img = mpimg.imread('seeds.png')\n",
    "\n",
    "def animation_plot(model, ax):\n",
    "    # Clear the axis to avoid over-plotting\n",
    "    ax.clear()\n",
    "\n",
    "    # Plot the grid using images\n",
    "    for (x, y), value in np.ndenumerate(model.np_grid):\n",
    "        if value == 1:  # Tractor\n",
    "            ax.imshow(tractor_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "        elif value == 3:  # Obstacle\n",
    "            ax.imshow(obstacle_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "        elif value == 2:  # Target\n",
    "            ax.imshow(target_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "        elif value == 4:  # Seeds\n",
    "            ax.imshow(seeds_img, extent=[y, y+1, x, x+1], aspect='auto')\n",
    "\n",
    "    # Add text for each tractor displaying the number of seeds it has\n",
    "    for agent in model.agents:\n",
    "        if agent.destroyed:\n",
    "            continue\n",
    "        ax.text(agent.pos[1] + 0.5, agent.pos[0] + 0.5, str(agent.seeds),\n",
    "                color='black', fontsize=12, ha='center', va='center', weight='bold')\n",
    "\n",
    "    # Fix axis limits based on the grid dimensions\n",
    "    ax.set_xlim([0, model.np_grid.shape[1]])\n",
    "    ax.set_ylim([0, model.np_grid.shape[0]])\n",
    "\n",
    "    # Set aspect ratio to 'equal' to prevent image stretching\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Set the title for the plot\n",
    "    ax.set_title(f\"Tractor model \\n Time-step: {model.t}, Collected: {model.Collected}\")\n",
    "\n",
    "# Example usage\n",
    "fig, ax = plt.subplots()\n",
    "model = TractorModel(tractorParameters)\n",
    "animation = ap.animate(model, fig, ax, animation_plot)\n",
    "animation.save('simulacionTractores.gif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the animation\n",
    "HTML(animation.to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

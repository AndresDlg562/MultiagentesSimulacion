{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sembradora 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It demonstrates how to use the [agentpy](https://agentpy.readthedocs.io) package to create and visualize networks, use the interactive module, and perform different types of sensitivity analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model design\n",
    "import agentpy as ap\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from queue import PriorityQueue\n",
    "from itertools import count\n",
    "import math\n",
    "\n",
    "# Visualization\n",
    "import imageio\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "# Guardar\n",
    "import pickle\n",
    "\n",
    "# Sistema\n",
    "import os\n",
    "\n",
    "# Machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "0 is empty\n",
    "1 is tractor\n",
    "2 is obstacle\n",
    "3 is target\n",
    "4 is seeds\n",
    "\"\"\"\n",
    "\n",
    "def is_connected(grid, free_positions):\n",
    "    \"\"\"Check if all free cells are connected using BFS.\"\"\"\n",
    "    n = grid.shape[0]\n",
    "    visited = set()\n",
    "    queue = deque([free_positions.pop()])\n",
    "    visited.add(queue[0])\n",
    "\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    \n",
    "    connected_count = 0\n",
    "    free_count = len(free_positions)\n",
    "\n",
    "    while queue:\n",
    "        x, y = queue.popleft()\n",
    "        for dx, dy in directions:\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            if (0 <= new_x < n and 0 <= new_y < n and \n",
    "                (new_x, new_y) in free_positions and (new_x, new_y) not in visited):\n",
    "                queue.append((new_x, new_y))\n",
    "                visited.add((new_x, new_y))\n",
    "                connected_count += 1\n",
    "                \n",
    "    return connected_count == free_count\n",
    "\n",
    "def is_adjacent(pos1, pos2):\n",
    "    return abs(pos1[0] - pos2[0]) <= 1 and abs(pos1[1] - pos2[1]) <= 1\n",
    "\n",
    "def generate_grid(model, n, obstacles_count, crops_count):\n",
    "    \"\"\"Generate a grid with obstacles, ensuring free cells are connected.\"\"\"\n",
    "    while True:\n",
    "        grid = ap.Grid(model, (n, n), track_empty=True)\n",
    "        grid.add_field(\"occupied\", 0)\n",
    "\n",
    "        obstacle_positions = set()\n",
    "        while len(obstacle_positions) < obstacles_count:\n",
    "            pos = (random.randint(0, n-1), random.randint(0, n-1))\n",
    "            if pos not in obstacle_positions:\n",
    "                # Check if pos is adjacent to any seed position\n",
    "                is_valid = True\n",
    "                for seed_pos in model.p.seedsPositions:\n",
    "                    if is_adjacent(pos, seed_pos):\n",
    "                        is_valid = False\n",
    "                        break\n",
    "                if is_valid:\n",
    "                    obstacle_positions.add(pos)\n",
    "\n",
    "        for pos in obstacle_positions:\n",
    "            grid[\"occupied\"][pos] = 1\n",
    "\n",
    "        free_positions = {(x, y) for x in range(n) for y in range(n) if grid[\"occupied\"][x, y] == 0}\n",
    "        \n",
    "        if is_connected(grid, free_positions):\n",
    "            break\n",
    "\n",
    "    for pos in obstacle_positions:\n",
    "        grid[\"occupied\"][pos] = 1\n",
    "        model.np_grid[pos] = 2\n",
    "        \n",
    "        crops_positions = set()\n",
    "        while len(crops_positions) < crops_count:\n",
    "            pos = (random.randint(0, n-1), random.randint(0, n-1))\n",
    "            if pos not in obstacle_positions and pos not in crops_positions:\n",
    "                crops_positions.add(pos)\n",
    "                model.np_grid[pos] = 3\n",
    "\n",
    "    agentlist = ap.AgentList(model, len(obstacle_positions), agent_type=2)\n",
    "    grid.add_agents(agentlist, obstacle_positions)\n",
    "    \n",
    "    model.grid = grid\n",
    "    \n",
    "    return obstacle_positions, crops_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CollectingTractor(ap.Agent):\n",
    "    def setup(self):\n",
    "        \"\"\"Initialize agent parameters.\"\"\"\n",
    "        self.initialized = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset agent attributes.\"\"\"\n",
    "        if not self.initialized:\n",
    "            self._initialize_agent()\n",
    "        else:\n",
    "            self._restore_agent()\n",
    "\n",
    "    def _initialize_agent(self):\n",
    "        \"\"\"Initialize the agent's initial state.\"\"\"\n",
    "        self.planted = 0\n",
    "        self.destroyed = False\n",
    "        self.seeds = 0\n",
    "        self.type = 1\n",
    "        self.past_pos = (0, 0)\n",
    "        self.pos = (0, 0)\n",
    "        self.new_pos = (0, 0)\n",
    "        self.visited_positions = set()\n",
    "        self.cell_state = 0 \n",
    "        self.last_action = None \n",
    "        self.last_planting_pos = None\n",
    "        self.steps_without_plant = 0\n",
    "        \n",
    "        # Initialize Q-learning parameters\n",
    "        self.q_table = np.zeros((self.p.grid_size, self.p.grid_size, self.p.capacity + 1, 7))\n",
    "        self.learning_rate = self.p.learning_rate\n",
    "        self.discount_factor = self.p.discount_factor\n",
    "        self.epsilon = self.p.epsilon\n",
    "\n",
    "        self.initialized = True  # Mark as initialized\n",
    "\n",
    "    def _restore_agent(self):\n",
    "        \"\"\"Restore the agent's state to its initial conditions.\"\"\"\n",
    "        self.planted = 0\n",
    "        self.targetIndex = 1\n",
    "        self.destroyed = False\n",
    "        self.seeds = 0\n",
    "        self.type = 1\n",
    "        self.past_pos = (0, 0)\n",
    "        self.pos = (0, 0)\n",
    "        self.new_pos = (0, 0)\n",
    "        self.visited_positions = set()\n",
    "        self.cell_state = 0\n",
    "        self.last_action = None\n",
    "        self.last_planting_pos = None\n",
    "        self.steps_without_plant = 0\n",
    "\n",
    "        # Initialize Q-learning parameters\n",
    "        self.q_table = np.zeros((self.p.grid_size, self.p.grid_size, self.p.capacity + 1, 7))\n",
    "        self.learning_rate = self.p.learning_rate\n",
    "        self.discount_factor = self.p.discount_factor\n",
    "        self.epsilon = self.p.epsilon\n",
    "        \n",
    "    def setCellState(self, pos):\n",
    "        if self.model.np_grid[pos] == 3:\n",
    "            self.cell_state = 0\n",
    "        elif self.model.np_grid[pos] == 4:\n",
    "            self.cell_state = 1\n",
    "        else:\n",
    "            self.cell_state = 2\n",
    "    \n",
    "    def exitGrid(self):\n",
    "        self.pos = (None, None)\n",
    "        \n",
    "    def setNewPositionVerify(self, pos):\n",
    "        self.new_pos = pos\n",
    "        \n",
    "    def move(self, agentes, dx=0, dy=0):\n",
    "        \"\"\"Move the tractor by (dx, dy).\"\"\"\n",
    "        x, y = self.pos\n",
    "        new_pos = (x + dx, y + dy)\n",
    "\n",
    "        # Asegúrate de que la nueva posición está dentro de los límites de la cuadrícula\n",
    "        if 0 <= new_pos[0] < self.p.grid_size and 0 <= new_pos[1] < self.p.grid_size:\n",
    "            # Verifica si la nueva posición no es un obstáculo\n",
    "            if self.model.np_grid[new_pos] != 2 and new_pos != self.p.seedsPositions[0]:\n",
    "                # Verifica si algún otro agente ya tiene la nueva posición como su siguiente posición\n",
    "                self.setNewPositionVerify(new_pos)\n",
    "                for agente in agentes:\n",
    "                    if agente is not self:\n",
    "                        if agente.new_pos == new_pos:\n",
    "                            return self.pos, False  # No se puede mover porque la posición está ocupada por otro agente\n",
    "                # Si no hay obstáculos ni agentes en la nueva posición, se mueve\n",
    "                self.past_pos = self.pos\n",
    "                self.pos = new_pos\n",
    "                self.setCellState(self.pos)\n",
    "                return self.pos, True\n",
    "        return self.pos, False  # No se puede mover porque está fuera de los límites o hay un obstáculo\n",
    "\n",
    "    def move_up(self, agents):\n",
    "        return self.move(agents, dy=1)\n",
    "\n",
    "    def move_down(self, agents):\n",
    "        return self.move(agents, dy=-1)\n",
    "\n",
    "    def move_left(self, agents):\n",
    "        return self.move(agents, dx=-1)\n",
    "\n",
    "    def move_right(self, agents):\n",
    "        return self.move(agents, dx=1)\n",
    "    \n",
    "    def exit(self, agents):\n",
    "        x, y = self.pos\n",
    "\n",
    "        # Verifica si el agente ya está en el borde del grid\n",
    "        if (x == 0 or x == self.p.grid_size - 1 or y == 0 or y == self.p.grid_size - 1) and 3 not in self.model.np_grid:\n",
    "            self.destroyed = True\n",
    "            return self.pos, True  \n",
    "        else:\n",
    "            return self.pos, False\n",
    "\n",
    "    def drop(self, agents):\n",
    "        \"\"\"Drop seeds at the current target position.\"\"\"\n",
    "        if self.seeds > 0 and self.pos and self.model.np_grid[self.pos] == 3:\n",
    "            self.seeds -= 1\n",
    "            # Remove target from grid\n",
    "            self.model.np_grid[self.pos] = 0\n",
    "            self.planted += 1\n",
    "            self.last_planting_pos = self.pos\n",
    "            return self.pos, True\n",
    "        else:\n",
    "            return self.pos, False\n",
    "\n",
    "    def collect(self, agents):\n",
    "        \"\"\"Collect seeds if available at the surrounding positions, including diagonals.\"\"\"\n",
    "        # Verificar si el agente está en alguna de las celdas adyacentes\n",
    "        if self.pos in self.model.neighbors:\n",
    "            if self.seeds == 0:\n",
    "                self.seeds = self.capacity\n",
    "                return self.pos, 1\n",
    "            elif self.seeds > 0 and self.seeds < self.capacity:\n",
    "                self.seeds = self.capacity\n",
    "                return self.pos, 2\n",
    "\n",
    "        # Si el agente no está en ninguna celda adyacente a seedsPositions\n",
    "        return self.pos, 3 \n",
    "\n",
    "    def get_reward(self, action, accionado):\n",
    "        \"\"\"Calculate the reward based on the action and outcome.\"\"\"\n",
    "        reward = -5  # Base penalty for movement to encourage efficiency\n",
    "\n",
    "        if action in [0, 1, 2, 3]:  # Movement actions\n",
    "            if accionado:\n",
    "                reward += -2  # Reduced penalty for valid movement\n",
    "            else:\n",
    "                reward += -10  # Strong penalty for hitting an obstacle or invalid move\n",
    "\n",
    "        elif action == 4:  # Drop seeds\n",
    "            if accionado:\n",
    "                reward += 1000  # High reward for planting seeds\n",
    "            else:\n",
    "                reward += -100  # General penalty for unsuccessful action\n",
    "            # Reward for planting near the last planting location\n",
    "            if self.last_planting_pos is not None:\n",
    "                last_x, last_y = self.last_planting_pos\n",
    "                current_x, current_y = self.pos\n",
    "                if abs(last_x - current_x) <= 1 and abs(last_y - current_y) <= 1:\n",
    "                    reward += 200  # Smaller reward for planting close to last planting\n",
    "\n",
    "        elif action == 5:  # Collect seeds\n",
    "            if accionado == 1:  # If no seeds\n",
    "                reward += 250  # Reward for collecting seeds\n",
    "            elif accionado == 2:  # If has seeds but not full\n",
    "                reward += 100  # Reward for collecting seeds when not full\n",
    "            else:  # If not adjacent to seedsPositions\n",
    "                reward += -150  # Penalty for missing seeds\n",
    "\n",
    "        elif action == 6:  # Exit\n",
    "            if accionado:\n",
    "                reward += -50\n",
    "            else:\n",
    "                reward += -200  \n",
    "\n",
    "        # Penalize for not planting seeds in a valid position\n",
    "        if self.model.np_grid[self.past_pos] == 3 and action != 4:\n",
    "            reward += -100  # Penalty for not planting when in a valid position\n",
    "\n",
    "        # Penalize for repeating the same action\n",
    "        if action in [4, 5] and self.last_action in [4, 5]:\n",
    "            if action == self.last_action:\n",
    "                reward -= 100\n",
    "            else:\n",
    "                reward += 0\n",
    "\n",
    "        # Penalize for repeating movements\n",
    "        if action in [0, 1, 2, 3] and self.last_action in [0, 1, 2, 3]:\n",
    "            if action == self.last_action:\n",
    "                reward += 0\n",
    "            else:\n",
    "                reward += -10\n",
    "\n",
    "        # Penalize for not moving\n",
    "        if action in [4, 5, 6]:\n",
    "            if self.pos == self.past_pos:\n",
    "                reward += -50  # Penalty for not moving to avoid stagnation\n",
    "            else:\n",
    "                reward += 0\n",
    "\n",
    "        # Reward for moving and collecting\n",
    "        if self.last_action in [0, 1, 2, 3] and accionado:\n",
    "            if action == 5 or action == 4:\n",
    "                reward += 100\n",
    "\n",
    "        for agente in self.model.agents:\n",
    "            if agente is not self and accionado:\n",
    "                x, y = agente.pos\n",
    "                if abs(x - self.pos[0]) >= 4  and abs(y - self.pos[1]) >= 4 :\n",
    "                    reward += 50  # Reward for being far from other agents\n",
    "        \n",
    "        # Penalize for not planting seeds after a certain number of steps\n",
    "        reward = reward - (2 * self.steps_without_plant)\n",
    "        \n",
    "        return reward\n",
    "\n",
    "        \n",
    "    def q_learning_update(self, state, action, reward, next_state):\n",
    "        pos_x, pos_y, seeds = state\n",
    "        next_pos_x, next_pos_y, next_seeds = next_state\n",
    "        \n",
    "        # Q-value actual considerando las celdas adyacentes\n",
    "        current_q = self.q_table[pos_x, pos_y, seeds, action]\n",
    "        # Max Q-value del siguiente estado\n",
    "        max_next_q = np.max(self.q_table[next_pos_x, next_pos_y, next_seeds])\n",
    "\n",
    "        # Actualizar el valor Q\n",
    "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
    "        self.q_table[pos_x, pos_y, seeds, action] = new_q\n",
    "        \n",
    "        if self.destroyed:\n",
    "            self.exitGrid()\n",
    "\n",
    "    def acciones(self, action, agents):\n",
    "        \"\"\"Map action to the corresponding method.\"\"\"\n",
    "        action_map = {\n",
    "            0: self.move_up,\n",
    "            1: self.move_down,\n",
    "            2: self.move_left,\n",
    "            3: self.move_right,\n",
    "            4: self.drop,\n",
    "            5: self.collect,\n",
    "            6: self.exit\n",
    "        }\n",
    "        if action != 4:\n",
    "            self.steps_without_plant += 1\n",
    "        accion = action_map[action]\n",
    "        return accion(agents)\n",
    "\n",
    "    def step(self, agents):\n",
    "        \"\"\"Execute a step in the agent's behavior.\"\"\"\n",
    "        self.planted = 0\n",
    "        if self.destroyed:\n",
    "            return \n",
    "            \n",
    "        # Definir el estado actual\n",
    "        state = (self.pos[0], self.pos[1], self.seeds)\n",
    "        \n",
    "        # Acción seleccionada usando epsilon-greedy\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice([0, 1, 2, 3, 4, 5, 6])\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[state[0], state[1], state[2]])\n",
    "            \n",
    "        # Ejecutar la acción y actualizar la Q-table\n",
    "        next_state, accionado = self.acciones(action, agents)\n",
    "        next_seeds = self.seeds  # Actualiza la cantidad de semillas después de la acción\n",
    "        \n",
    "        # Asegúrate de que next_state esté dentro de los límites\n",
    "        next_state = (next_state[0], next_state[1], next_seeds)\n",
    "        reward = self.get_reward(action, accionado)\n",
    "        self.q_learning_update(state, action, reward, next_state)\n",
    "        \n",
    "        # Plantado\n",
    "        planted = self.planted\n",
    "        \n",
    "        # Save last action\n",
    "        self.last_action = action\n",
    "        \n",
    "        return reward, planted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TractorModel(ap.Model):\n",
    "    def setup(self):\n",
    "        \"\"\"Initialize the environment and generate coordinates.\"\"\"\n",
    "        self.initialized = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset or initialize the model state.\"\"\"\n",
    "        if not self.initialized:\n",
    "            self._initialize_environment()\n",
    "        else:\n",
    "            self._restore_environment()\n",
    "\n",
    "        self.steps = 0  # Reset the step counter\n",
    "\n",
    "    def _initialize_environment(self):\n",
    "        \"\"\"Initialize the environment, agents, and their targets.\"\"\"\n",
    "        self.steps = 0\n",
    "        self.max_steps = self.p.max_steps\n",
    "        self.np_grid = np.zeros((self.p.grid_size, self.p.grid_size))\n",
    "\n",
    "        # Generate and store obstacle positions\n",
    "        obastacle_pos, crop_pos = generate_grid(self, self.p.grid_size, self.p.obstacles_count, self.p.crops_count)\n",
    "        self.obstacle_positions = np.copy(self.grid[\"occupied\"])\n",
    "        self.crops_pos = crop_pos   \n",
    "        \n",
    "        # Create agents and assign initial properties\n",
    "        self.agents = ap.AgentList(self, self.p.number_of_tractors, CollectingTractor)\n",
    "        self._initialize_agents(obastacle_pos, crop_pos)\n",
    "\n",
    "        self.grid.add_agents(self.agents, [tractor.pos for tractor in self.agents])\n",
    "        \n",
    "        for seed in self.p.seedsPositions:\n",
    "            self.np_grid[seed] = 4\n",
    "            \n",
    "        x, y = self.p.seedsPositions[0]\n",
    "        self.neighbors = [\n",
    "            (x-1, y), (x+1, y), (x, y-1), (x, y+1),  \n",
    "            (x-1, y-1), (x-1, y+1), (x+1, y-1), (x+1, y+1)  \n",
    "        ]\n",
    "        \n",
    "        for pos in self.neighbors:\n",
    "            self.np_grid[pos] = 0\n",
    "        \n",
    "        self.initialized = True  # Mark as initialized\n",
    "        \n",
    "    def _restore_environment(self):\n",
    "        \"\"\"Restore the environment to its initial state.\"\"\"\n",
    "        self.steps = 0\n",
    "        self.max_steps = self.p.max_steps\n",
    "        self.np_grid = np.zeros((self.p.grid_size, self.p.grid_size))\n",
    "        \n",
    "        # Restore obstacle positions\n",
    "        for x in range(self.p.grid_size):\n",
    "            for y in range(self.p.grid_size):\n",
    "                if self.obstacle_positions[x, y] == 1:\n",
    "                    self.np_grid[x, y] = 2 \n",
    "        \n",
    "        # Restore crop positions\n",
    "        for pos in self.crops_pos:\n",
    "            self.np_grid[pos] = 3\n",
    "            \n",
    "        self.agents.capacity = self.p.capacity\n",
    "        self.agents.seeds = self.p.starting_seeds\n",
    "        self.agents.destroyed = False\n",
    "        self.agents.steps_without_plant = 0\n",
    "        # Asignar las posiciones a los tractores\n",
    "        for tractor, position in zip(self.agents, self.p.tractor_positions):\n",
    "            tractor.pos = position\n",
    "            \n",
    "        self.grid.add_agents(self.agents, [tractor.pos for tractor in self.agents])\n",
    "\n",
    "        for seed in self.p.seedsPositions:\n",
    "            self.np_grid[seed] = 4\n",
    "            \n",
    "        x, y = self.p.seedsPositions[0]\n",
    "        self.neighbors = [\n",
    "            (x-1, y), (x+1, y), (x, y-1), (x, y+1),  \n",
    "            (x-1, y-1), (x-1, y+1), (x+1, y-1), (x+1, y+1)  \n",
    "        ]\n",
    "        \n",
    "        for pos in self.neighbors:\n",
    "            self.np_grid[pos] = 0\n",
    "\n",
    "    def _initialize_agents(self, obstacle_pos, crop_pos):\n",
    "        \"\"\"Initialize agents with positions and targets.\"\"\"\n",
    "        self.agents.capacity = self.p.capacity\n",
    "        self.agents.seeds = self.p.starting_seeds\n",
    "        self.agents.destroyed = False\n",
    "        # Asignar las posiciones a los tractores\n",
    "        for tractor, position in zip(self.agents, self.p.tractor_positions):\n",
    "            tractor.pos = position\n",
    "\n",
    "        self.coordsUsed = set()\n",
    "        self.coordsUsed.update(obstacle_pos)\n",
    "        self.coordsUsed.update(crop_pos)\n",
    "        self.targets_by_tractor = []\n",
    "        \n",
    "    def _get_free_position(self):\n",
    "        \"\"\"Get a free position on the grid.\"\"\"\n",
    "        while True:\n",
    "            x, y = random.randint(0, self.p.grid_size - 1), random.randint(0, self.p.grid_size - 1)\n",
    "            if self.grid[\"occupied\"][(x, y)] == 0 and (x, y) not in self.coordsUsed and (x, y) != (0, 0):\n",
    "                return (x, y)\n",
    "\n",
    "    def step(self):\n",
    "        total_reward = 0\n",
    "        total_planted = 0\n",
    "         # Filtra los tractores activos\n",
    "        active_tractors = [tractor for tractor in self.agents if not tractor.destroyed]\n",
    "        \n",
    "        for tractor in active_tractors:\n",
    "            reward, planted = tractor.step(self.agents)\n",
    "            total_reward += reward\n",
    "            total_planted += planted\n",
    "        if self.steps >= self.max_steps:\n",
    "            for tractor in self.agents:\n",
    "                tractor.destroyed = True\n",
    "            self.end()\n",
    "        self.steps += 1\n",
    "        return total_reward, total_planted\n",
    "\n",
    "    def end(self):\n",
    "        self.report('Total targets', self.agents.collected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las imágenes una vez\n",
    "tractor_img = mpimg.imread('tractor.png')\n",
    "obstacle_img = mpimg.imread('obstacle.png')\n",
    "target_img = mpimg.imread('target.png')\n",
    "seeds_img = mpimg.imread('seeds.png')\n",
    "empty_img = mpimg.imread('empty.png')\n",
    "\n",
    "# Diccionario de imágenes\n",
    "images = {\n",
    "    'tractor': tractor_img,\n",
    "    'obstacle': obstacle_img,\n",
    "    'target': target_img,\n",
    "    'seeds': seeds_img,\n",
    "    'empty': empty_img\n",
    "}\n",
    "\n",
    "def save_frame(model, filename, total_reward, total_planted, images):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Asignar imágenes basadas en los valores de la cuadrícula\n",
    "    img_dict = {0: images['empty'], 2: images['obstacle'], 3: images['target'], 4: images['seeds']}\n",
    "    \n",
    "    # Mostrar la cuadrícula\n",
    "    for (x, y), value in np.ndenumerate(model.np_grid):\n",
    "        if value in img_dict:\n",
    "            ax.imshow(img_dict[value], extent=[y, y+1, x, x+1], aspect='auto')\n",
    "            \n",
    "    # Añadir tractores y texto\n",
    "    for agent in model.agents:\n",
    "        if agent.destroyed:\n",
    "            continue\n",
    "        ax.imshow(images['tractor'], extent=[agent.pos[1], agent.pos[1]+1, agent.pos[0], agent.pos[0]+1], aspect='auto')\n",
    "        ax.text(agent.pos[1] + 0.5, agent.pos[0] + 0.5, str(agent.seeds),\n",
    "                color='black', fontsize=12, ha='center', va='center', weight='bold')\n",
    "\n",
    "    # Configurar límites y aspecto\n",
    "    ax.set_xlim([0, model.np_grid.shape[1]])\n",
    "    ax.set_ylim([0, model.np_grid.shape[0]])\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Título del gráfico\n",
    "    ax.set_title(f\"Tractor model \\n Reward: {total_reward} Planted: {total_planted}\")\n",
    "\n",
    "    # Guardar el cuadro\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def create_gif(filenames, gif_filename):\n",
    "    with imageio.get_writer(gif_filename, mode='I', duration=0.1) as writer:\n",
    "        for filename in filenames:\n",
    "            writer.append_data(imageio.imread(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_crops_count(grid_size, obstacles_count):\n",
    "    return grid_size**2 - obstacles_count\n",
    "\n",
    "tractorParameters = { # NO MOVER NINGÚN PARÁMETRO\n",
    "    'grid_size': 11,\n",
    "    'obstacles_count': 7,\n",
    "    'number_of_tractors': 4,\n",
    "    'tractor_positions': [(4, 5), (5, 4), (6, 5), (5, 6)],\n",
    "    'max_steps': 150,\n",
    "    'seedsPositions': [(5, 5)],\n",
    "    'capacity': 5,\n",
    "    'starting_seeds': 5,\n",
    "    'learning_rate': 0.1, # No mover\n",
    "    'discount_factor': 0.9, # No mover\n",
    "    'epsilon': 1.0,\n",
    "    'crops_count': calculate_crops_count(11, 7)  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "rewards_per_episode = []\n",
    "epsilon_values = []\n",
    "planting_per_episode = []\n",
    "\n",
    "# Inicializar el modelo\n",
    "model = TractorModel(tractorParameters)\n",
    "model.setup()\n",
    "\n",
    "num_episodes = 12500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(episode, save_gif=False):\n",
    "    model.reset()\n",
    "    total_reward = 0\n",
    "    total_planted = 0\n",
    "\n",
    "    # Configura epsilon basado en el progreso de los episodios\n",
    "    epsilon_start, epsilon_end = 1.0, 0.0\n",
    "    if episode == num_episodes - 1:\n",
    "        epsilon = epsilon_end\n",
    "    elif episode < 0.00 * num_episodes:\n",
    "        epsilon = 1.0\n",
    "    else:\n",
    "        epsilon = max(epsilon_end, epsilon_start - (epsilon_start - epsilon_end) * episode / num_episodes)\n",
    "\n",
    "    # Asigna epsilon a cada tractor\n",
    "    for tractor in model.agents:\n",
    "        tractor.epsilon = epsilon\n",
    "\n",
    "    filenames = []\n",
    "    max_steps = model.max_steps  # Acceder una vez al valor de max_steps\n",
    "\n",
    "    with TemporaryDirectory() as temp_dir:\n",
    "        while model.steps < max_steps:\n",
    "            reward, planted = model.step()  # Ejecutar un paso en el modelo\n",
    "            total_reward += reward\n",
    "            total_planted += planted\n",
    "\n",
    "            if all(tractor.destroyed for tractor in model.agents):\n",
    "                break\n",
    "\n",
    "            if save_gif:\n",
    "                filename = os.path.join(temp_dir, f'frame_{model.steps}.png')\n",
    "                save_frame(model, filename, total_reward, total_planted, images)\n",
    "                filenames.append(filename)\n",
    "\n",
    "        if save_gif:\n",
    "            gif_filename = '1rstSimulacionTractores.gif' if episode == 0 else 'lastSimulacionTractores.gif'\n",
    "            create_gif(filenames, gif_filename)\n",
    "\n",
    "    return total_reward, epsilon, total_planted\n",
    "\n",
    "# Bucle principal de entrenamiento\n",
    "for episode in range(num_episodes):\n",
    "    save_gif = (episode == 0 or episode == num_episodes - 1)\n",
    "    total_reward, epsilon, total_planted = run_episode(episode, save_gif)\n",
    "\n",
    "    rewards_per_episode.append(total_reward)\n",
    "    epsilon_values.append(epsilon)\n",
    "    planting_per_episode.append(total_planted)\n",
    "\n",
    "    # Monitorear el progreso cada 10 episodios\n",
    "    if episode == num_episodes - 1 or episode % 100 == 0:\n",
    "        print(f'Episodio {episode + 1}/{num_episodes}, Recompensa total: {total_reward:.2f}, Epsilon: {epsilon:.2f}, Plantados: {total_planted:.2f}')\n",
    "        \n",
    "# Guardar cada tabla Q en un archivo con distintos nombres\n",
    "for i, tractor in enumerate(model.agents):\n",
    "    with open(f'q_table_{i}.pkl', 'wb') as f:\n",
    "        pickle.dump(tractor.q_table, f)\n",
    "\n",
    "print(\"Tablas Q guardadas exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que rewards_per_episode y planting_per_episode sean arrays de NumPy\n",
    "rewards_per_episode = np.array(rewards_per_episode)\n",
    "planting_per_episode = np.array(planting_per_episode)\n",
    "\n",
    "# Colores para las gráficas\n",
    "color_reward = '#98FB98'  # Verde menta\n",
    "color_trend_line = '#FF6F61'  # Coral\n",
    "color_smoothed_planting = '#000'  # Negro\n",
    "color_epsilon = '#4682B4'  # Azul acero\n",
    "\n",
    "# Ajustar una línea recta (polinomio de grado 1) a los datos de recompensas\n",
    "z_rewards = np.polyfit(range(len(rewards_per_episode)), rewards_per_episode, 1)\n",
    "p_rewards = np.poly1d(z_rewards)\n",
    "\n",
    "# Crear la gráfica de recompensas\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(rewards_per_episode, label='Total Reward per Episode', color=color_reward)\n",
    "plt.plot(range(len(rewards_per_episode)), p_rewards(range(len(rewards_per_episode))), \"r--\", label='Trend Line', color=color_trend_line)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Reward per Episode')\n",
    "plt.legend()\n",
    "\n",
    "# Suavizar la serie de datos de plantación de semillas utilizando un promedio móvil\n",
    "window_size = 50\n",
    "smoothed_planting = np.convolve(planting_per_episode, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Ajustar una línea recta (polinomio de grado 1) a los datos de plantación suavizados\n",
    "z_planting = np.polyfit(range(len(smoothed_planting)), smoothed_planting, 1)\n",
    "p_planting = np.poly1d(z_planting)\n",
    "\n",
    "# Crear la gráfica de plantación de semillas\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(planting_per_episode, label='Planting per Episode', color=color_reward)\n",
    "plt.plot(range(len(smoothed_planting)), smoothed_planting, label='Smoothed Planting', color=color_smoothed_planting)\n",
    "plt.plot(range(len(smoothed_planting)), p_planting(range(len(smoothed_planting))), \"r--\", label='Trend Line', color=color_trend_line)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Planted')\n",
    "plt.title('Planting per Episode')\n",
    "plt.legend()\n",
    "\n",
    "# Crear la gráfica de epsilon\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epsilon_values, label='Epsilon', color=color_epsilon)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.title('Epsilon Decay')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí va una red capaz de aprender a jugar al juego de los tractores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulación Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el modelo\n",
    "model.reset()\n",
    "\n",
    "# Steps\n",
    "model.max_steps = 600\n",
    "\n",
    "# Establecer epsilon a 0 para la simulación final\n",
    "epsilon = 0\n",
    "for tractor in model.agents:\n",
    "    tractor.epsilon = epsilon\n",
    "\n",
    "# Cargar las tablas Q completadas para cada tractor\n",
    "for i, tractor in enumerate(model.agents):\n",
    "    with open(f'q_table_{i}.pkl', 'rb') as f:\n",
    "        tractor.q_table = pickle.load(f)\n",
    "\n",
    "# Variables para almacenar recompensas y plantados\n",
    "total_reward = 0\n",
    "total_planted = 0\n",
    "filenames = []\n",
    "\n",
    "# Directorio temporal para almacenar cuadros\n",
    "with TemporaryDirectory() as temp_dir:\n",
    "    while model.steps < model.max_steps:\n",
    "        reward, planted = model.step()  # Ejecutar un paso en el modelo\n",
    "        total_reward += reward\n",
    "        total_planted += planted\n",
    "\n",
    "        # Comprobar si todos los tractores están destruidos\n",
    "        if all(tractor.destroyed for tractor in model.agents):\n",
    "            break\n",
    "\n",
    "        # Guardar cuadro del estado actual del modelo\n",
    "        filename = os.path.join(temp_dir, f'frame_{model.steps}.png')\n",
    "        save_frame(model, filename, total_reward, total_planted, images)\n",
    "        filenames.append(filename)\n",
    "\n",
    "        # Imprimir progreso de la simulación\n",
    "        print(f\"Numero de pasos: {model.steps}\")\n",
    "        print(f\"Recompensa total: {total_reward:.2f}, Plantados: {total_planted:.2f}\")\n",
    "\n",
    "    # Crear GIF de la simulación final\n",
    "    gif_filename = 'simulacionFinal.gif'\n",
    "    create_gif(filenames, gif_filename)\n",
    "\n",
    "print(\"Simulación final guardada exitosamente en 'simulacionFinal.gif'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
